{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding style guide"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#https://www.python.org/dev/peps/pep-0008/\n",
    "\n",
    "### Use ‘single’ quotes and snake_case unless there is a good reason not to\n",
    "\n",
    "### Make all module imports at the top of the notebook\n",
    "\n",
    "\n",
    "### Use standard shorthands for common modules and avoid * imports:\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "```\n",
    "\n",
    "### Define all existing tables at the top after module imports e.g.\n",
    "prods = spark.table('ddp_central.src_sdw_prd_productstructgpx')\n",
    "roles = spark.table('ddp_central.src_sdw_agreementcustomerrole')\n",
    "agreements = spark.table('ddp_central.src_sdw_agr_customeragreement')\n",
    "\n",
    "\n",
    "### Avoid using SQL statements where pyspark is possible\n",
    "\n",
    "//Avoid\n",
    "agreementids = spark.sql('select agreementid from ddp_central.src_sdw_agr_customeragreement')\n",
    "\n",
    "//Instead use\n",
    "agreements = spark.table('ddp_central.src_sdw_agr_customeragreement')\n",
    "...\n",
    "agreementids = agreements.select('agreementid')\n",
    "\n",
    "### Use separate cells for lazy transformations and for actions when possible\n",
    "\n",
    "\n",
    "# Aligned with opening delimiter.\n",
    "foo = long_function_name(var_one, var_two,\n",
    "                         var_three, var_four)\n",
    "\n",
    "# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.\n",
    "def long_function_name(\n",
    "        var_one, var_two, var_three,\n",
    "        var_four):\n",
    "    print(var_one)\n",
    "\n",
    "# Hanging indents should add a level.\n",
    "foo = long_function_name(\n",
    "    var_one, var_two,\n",
    "    var_three, var_four)\n",
    "#If operators with different priorities are used, consider adding whitespace around the operators with the lowest priority(ies). \n",
    "# Use your own judgment; however, never use more than one space, \n",
    "# and always have the same amount of whitespace on both sides of a binary operator.\n",
    "i = i + 1\n",
    "submitted += 1\n",
    "x = x*2 - 1\n",
    "hypot2 = x*x + y*y\n",
    "c = (a+b) * (a-b)\n",
    "\n",
    "def munge(input: AnyStr): ...\n",
    "def munge() -> AnyStr: ...\n",
    "\n",
    "def complex(real, imag=0.0):\n",
    "    return magic(r=real, i=imag)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "All logic should be in a Try-Catch block and exceptions should be logged. Python offers its own logging utility as well but it’s just about printing the entire stake trace to the file. This way if a Spark job fails one can always look at the logs later to see what went wrong.\n",
    "\n",
    "def Do_Something():\n",
    "    try:\n",
    "        i+=1\n",
    "        return 1\n",
    "    except Exception as e:    \n",
    "        Logger(\"Exception Trace: \", e)\n",
    "        traceback.print_exc()\n",
    "        Logger(\"Errored out during setting up population\")\n",
    "        Logger(traceback.format_exc())\n",
    "        return -1\n",
    "\n",
    "\n",
    "Use global variables in python and cache them for better memory utilization. Clear cache before big operations to make sure there isn’t any unnecessary code cached.\n",
    "\n",
    "my_df = spark.table(“table”).cache() \n",
    "\n",
    "def Do_Something():\n",
    "    try:\n",
    "        global my_df\n",
    "        my_df = my_df.withColumn(“this”, “that”)\n",
    "        \n",
    "        #Clear Cache\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "        #Big Operation\n",
    "            \n",
    "        return 1\n",
    "    except Exception as e:    \n",
    "        Logger(\"Exception Trace: \", e)\n",
    "        traceback.print_exc()\n",
    "        Logger(\"Errored out during setting up population\")\n",
    "        Logger(traceback.format_exc())\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://data-flair.training/blogs/python-tutorials-home/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://docs.python.org/3.3/tutorial/errors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "colors": "BasicMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "profile": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation}\n",
       "   E = mc^2\n",
       "\\end{equation}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{equation}\n",
    "   E = mc^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/data/home/p901cyo/Python Learning'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "    <HEAD>\n",
       "        <TITLE>Your Title Here</TITLE>\n",
       "    </HEAD>\n",
       "<BODY BGCOLOR=\"FFFFFF\">\n",
       "    <HR>\n",
       "        <a href=\"http://somegreatsite.com\">Link Name</a> is a link to another nifty site\n",
       "        <H1>This is a Header</H1>\n",
       "        <H2>This is a Medium Header</H2>\n",
       "        Send me mail at <a href=\"mailto:anna.baecklund@swedbank.se\">\n",
       "        anna.baecklund@swedbank.se</a>.\n",
       "        <P> This is a new paragraph!\n",
       "        <P> <B>This is a new paragraph!</B>\n",
       "        <BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
       "    <HR>\n",
       "</BODY>\n",
       "</HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.w3schools.com/html/\n",
    "%%html\n",
    "<HTML>\n",
    "    <HEAD>\n",
    "        <TITLE>Your Title Here</TITLE>\n",
    "    </HEAD>\n",
    "<BODY BGCOLOR=\"FFFFFF\">\n",
    "    <HR>\n",
    "        <a href=\"http://somegreatsite.com\">Link Name</a> is a link to another nifty site\n",
    "        <H1>This is a Header</H1>\n",
    "        <H2>This is a Medium Header</H2>\n",
    "        Send me mail at <a href=\"mailto:anna.baecklund@swedbank.se\">\n",
    "        anna.baecklund@swedbank.se</a>.\n",
    "        <P> This is a new paragraph!\n",
    "        <P> <B>This is a new paragraph!</B>\n",
    "        <BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
    "    <HR>\n",
    "</BODY>\n",
    "</HTML>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "absl-py (0.6.1)\n",
      "alabaster (0.7.10)\n",
      "anaconda-client (1.6.9)\n",
      "anaconda-navigator (1.7.0)\n",
      "anaconda-project (0.8.2)\n",
      "asn1crypto (0.24.0)\n",
      "astor (0.7.1)\n",
      "astroid (1.6.1)\n",
      "astropy (2.0.3)\n",
      "attrs (19.1.0)\n",
      "audioread (2.1.5)\n",
      "Babel (2.5.3)\n",
      "backports-abc (0.5)\n",
      "backports.functools-lru-cache (1.4)\n",
      "backports.shutil-get-terminal-size (1.0.0)\n",
      "backports.ssl-match-hostname (3.5.0.1)\n",
      "backports.weakref (1.0.post1)\n",
      "beautifulsoup4 (4.6.0)\n",
      "bitarray (0.8.1)\n",
      "bkcharts (0.2)\n",
      "blaze (0.11.3)\n",
      "bleach (2.1.2)\n",
      "blis (0.2.4)\n",
      "bokeh (0.12.13)\n",
      "boto (2.48.0)\n",
      "boto3 (1.9.176)\n",
      "botocore (1.12.176)\n",
      "Bottleneck (1.2.1)\n",
      "bs4 (0.0.1)\n",
      "bz2file (0.98)\n",
      "cached-property (1.4.0)\n",
      "cdecimal (2.3)\n",
      "certifi (2019.6.16)\n",
      "cffi (1.11.4)\n",
      "chardet (3.0.4)\n",
      "cld2-cffi (0.1.4)\n",
      "click (6.7)\n",
      "cloudpickle (0.5.2)\n",
      "clyent (1.2.2)\n",
      "colorama (0.3.9)\n",
      "conda (4.5.4)\n",
      "conda-build (3.4.1)\n",
      "conda-verify (2.0.0)\n",
      "configparser (3.5.0)\n",
      "contextlib2 (0.5.5)\n",
      "coverage (4.5.1)\n",
      "cryptography (2.1.4)\n",
      "cycler (0.10.0)\n",
      "cymem (2.0.2)\n",
      "Cython (0.27.3)\n",
      "cytoolz (0.9.0)\n",
      "dask (0.16.1)\n",
      "datashape (0.5.4)\n",
      "decorator (4.2.1)\n",
      "dill (0.2.9)\n",
      "distributed (1.20.2)\n",
      "docutils (0.14)\n",
      "ds-lime (0.1.1.27)\n",
      "eli5 (0.8.1)\n",
      "entrypoints (0.2.3)\n",
      "enum34 (1.1.6)\n",
      "estnltk (1.3+stacc)\n",
      "et-xmlfile (1.0.1)\n",
      "fastcache (1.0.2)\n",
      "filelock (2.0.13)\n",
      "Flask (0.12.2)\n",
      "Flask-Cors (3.0.3)\n",
      "funcsigs (1.0.2)\n",
      "functools32 (3.2.3.post2)\n",
      "funcy (1.10.1)\n",
      "future (0.16.0)\n",
      "futures (3.2.0)\n",
      "fuzzywuzzy (0.16.0)\n",
      "gast (0.2.0)\n",
      "gensim (3.7.3)\n",
      "gevent (1.2.2)\n",
      "glob2 (0.6)\n",
      "gmpy2 (2.0.8)\n",
      "graphframes (0.6)\n",
      "graphviz (0.10.1)\n",
      "greenlet (0.4.12)\n",
      "grin (1.2.1)\n",
      "grpcio (1.17.1)\n",
      "h5py (2.7.1)\n",
      "heapdict (1.0.0)\n",
      "html5lib (1.0.1)\n",
      "hypothesis (3.50.2)\n",
      "idna (2.8)\n",
      "imageio (2.2.0)\n",
      "imagesize (0.7.1)\n",
      "ipaddress (1.0.19)\n",
      "ipykernel (4.8.0)\n",
      "ipython (5.4.1)\n",
      "ipython-genutils (0.2.0)\n",
      "ipywidgets (7.1.1)\n",
      "isort (4.2.15)\n",
      "itsdangerous (0.24)\n",
      "jdcal (1.3)\n",
      "jedi (0.11.1)\n",
      "Jinja2 (2.10)\n",
      "jmespath (0.9.4)\n",
      "joblib (0.11)\n",
      "jsonschema (3.0.1)\n",
      "jupyter (1.0.0)\n",
      "jupyter-client (5.2.2)\n",
      "jupyter-console (5.2.0)\n",
      "jupyter-core (4.4.0)\n",
      "jupyterlab (0.31.5)\n",
      "jupyterlab-launcher (0.10.2)\n",
      "Keras (2.2.4)\n",
      "Keras-Applications (1.0.6)\n",
      "Keras-Preprocessing (1.0.5)\n",
      "kiwisolver (1.0.1)\n",
      "langid (1.1.6)\n",
      "lazy-object-proxy (1.3.1)\n",
      "librosa (0.6.0)\n",
      "lime (0.1.1.29)\n",
      "llvmlite (0.21.0)\n",
      "locket (0.2.0)\n",
      "lorem-ipsum-generator (0.3)\n",
      "lxml (4.1.1)\n",
      "Markdown (3.0.1)\n",
      "MarkupSafe (1.0)\n",
      "matplotlib (2.2.2)\n",
      "mccabe (0.6.1)\n",
      "mistune (0.8.3)\n",
      "mkl-fft (1.0.0)\n",
      "mkl-random (1.0.1)\n",
      "mock (2.0.0)\n",
      "mpmath (1.0.0)\n",
      "msgpack-numpy (0.4.4.2)\n",
      "msgpack-python (0.5.1)\n",
      "multipledispatch (0.4.9)\n",
      "multiprocess (0.70.6.1)\n",
      "murmurhash (1.0.2)\n",
      "navigator-updater (0.1.0)\n",
      "nbconvert (5.3.1)\n",
      "nbformat (4.4.0)\n",
      "networkx (2.1)\n",
      "nltk (3.2.5)\n",
      "nose (1.3.7)\n",
      "notebook (5.4.0)\n",
      "numba (0.36.2+0.g540650dbc.dirty)\n",
      "numexpr (2.6.4)\n",
      "numpy (1.16.4)\n",
      "numpydoc (0.7.0)\n",
      "odo (0.5.1)\n",
      "olefile (0.45.1)\n",
      "openpyxl (2.4.10)\n",
      "packaging (16.8)\n",
      "pandas (0.24.2)\n",
      "pandoc (1.0.2)\n",
      "pandocfilters (1.4.2)\n",
      "parso (0.1.1)\n",
      "partd (0.3.8)\n",
      "path.py (10.5)\n",
      "pathlib (1.0.1)\n",
      "pathlib2 (2.3.0)\n",
      "patsy (0.5.0)\n",
      "pbr (5.1.1)\n",
      "pep8 (1.7.1)\n",
      "pexpect (4.3.1)\n",
      "pickleshare (0.7.4)\n",
      "Pillow (5.0.0)\n",
      "pip (9.0.1)\n",
      "pkginfo (1.4.1)\n",
      "plac (0.9.6)\n",
      "plotly (3.5.0)\n",
      "pluggy (0.6.0)\n",
      "ply (3.10)\n",
      "preshed (2.0.1)\n",
      "prompt-toolkit (1.0.15)\n",
      "protobuf (3.6.1)\n",
      "psutil (5.4.3)\n",
      "ptyprocess (0.5.2)\n",
      "py (1.5.2)\n",
      "pycairo (1.15.4)\n",
      "pycodestyle (2.3.1)\n",
      "pycosat (0.6.3)\n",
      "pycparser (2.18)\n",
      "pycrypto (2.6.1)\n",
      "pycurl (7.43.0.1)\n",
      "pydotplus (2.0.2)\n",
      "pyflakes (1.6.0)\n",
      "Pygments (2.2.0)\n",
      "pyLDAvis (2.1.1)\n",
      "pylint (1.8.2)\n",
      "pyodbc (4.0.22)\n",
      "pyOpenSSL (17.5.0)\n",
      "pyparsing (2.2.0)\n",
      "pyrsistent (0.15.2)\n",
      "PyScaffold (2.5.8)\n",
      "PySocks (1.6.7)\n",
      "pytest (3.3.2)\n",
      "pytest-runner (4.2)\n",
      "python-crfsuite (0.9.5)\n",
      "python-dateutil (2.6.1)\n",
      "pytz (2017.3)\n",
      "PyWavelets (0.5.2)\n",
      "PyYAML (3.12)\n",
      "pyzmq (16.0.3)\n",
      "QtAwesome (0.4.4)\n",
      "qtconsole (4.3.1)\n",
      "QtPy (1.3.1)\n",
      "regex (2018.2.21)\n",
      "requests (2.22.0)\n",
      "resampy (0.2.0)\n",
      "retrying (1.3.3)\n",
      "rope (0.10.7)\n",
      "ruamel-yaml (0.15.35)\n",
      "s3transfer (0.2.1)\n",
      "scandir (1.6)\n",
      "scikit-image (0.14.0)\n",
      "scikit-learn (0.19.1)\n",
      "scipy (1.0.1)\n",
      "seaborn (0.8.1)\n",
      "Send2Trash (1.4.2)\n",
      "setuptools (41.0.1)\n",
      "shap (0.26.0)\n",
      "simplegeneric (0.8.1)\n",
      "singledispatch (3.4.0.3)\n",
      "six (1.12.0)\n",
      "skater (1.1.2)\n",
      "smart-open (1.8.4)\n",
      "snorkel (0.0.12)\n",
      "snowballstemmer (1.2.1)\n",
      "sortedcollections (0.5.3)\n",
      "sortedcontainers (1.5.9)\n",
      "spacy (2.1.4)\n",
      "Sphinx (1.6.6)\n",
      "sphinxcontrib-websupport (1.0.1)\n",
      "spyder (3.2.6)\n",
      "SQLAlchemy (1.2.1)\n",
      "srsly (0.0.7)\n",
      "statsmodels (0.9.0)\n",
      "subprocess32 (3.2.7)\n",
      "sympy (1.1.1)\n",
      "tables (3.4.2)\n",
      "tabulate (0.8.2)\n",
      "tblib (1.3.2)\n",
      "tensorboard (1.12.0)\n",
      "tensorflow (1.12.0)\n",
      "teradata (15.10.0.21)\n",
      "termcolor (1.1.0)\n",
      "terminado (0.8.1)\n",
      "testpath (0.3.1)\n",
      "textblob (0.15.1)\n",
      "thinc (7.0.4)\n",
      "toolz (0.9.0)\n",
      "torch (0.4.1)\n",
      "torchtext (0.2.3)\n",
      "torchvision (0.2.1)\n",
      "tornado (4.5.3)\n",
      "tqdm (4.32.2)\n",
      "traitlets (4.3.2)\n",
      "transliterate (1.10)\n",
      "tsfresh (0.11.1)\n",
      "typing (3.6.2)\n",
      "ujson (1.35)\n",
      "unicodecsv (0.14.1)\n",
      "Unidecode (1.0.22)\n",
      "urllib3 (1.25.3)\n",
      "wasabi (0.2.2)\n",
      "wcwidth (0.1.7)\n",
      "webencodings (0.5.1)\n",
      "Werkzeug (0.14.1)\n",
      "wheel (0.30.0)\n",
      "widgetsnbextension (3.1.0)\n",
      "wordcloud (1.3.1)\n",
      "wrapt (1.10.11)\n",
      "xgboost (0.81)\n",
      "xlrd (1.1.0)\n",
      "XlsxWriter (1.0.2)\n",
      "xlwt (1.3.0)\n",
      "yellowbrick (0.6)\n",
      "zict (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "# fun things to try with jupyter notebook. https://www.zhihu.com/question/46309360/answer/742984819\n",
    "#check what packages are available\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absl-py==0.6.1', 'alabaster==0.7.10', 'anaconda-client==1.6.9', 'anaconda-navigator==1.7.0', 'anaconda-project==0.8.2', 'asn1crypto==0.24.0', 'astor==0.7.1', 'astroid==1.6.1', 'astropy==2.0.3', 'attrs==19.1.0', 'audioread==2.1.5', 'babel==2.5.3', 'backports-abc==0.5', 'backports.functools-lru-cache==1.4', 'backports.shutil-get-terminal-size==1.0.0', 'backports.ssl-match-hostname==3.5.0.1', 'backports.weakref==1.0.post1', 'beautifulsoup4==4.6.0', 'bitarray==0.8.1', 'bkcharts==0.2', 'blaze==0.11.3', 'bleach==2.1.2', 'blis==0.2.4', 'bokeh==0.12.13', 'boto3==1.9.176', 'boto==2.48.0', 'botocore==1.12.176', 'bottleneck==1.2.1', 'bs4==0.0.1', 'bz2file==0.98', 'cached-property==1.4.0', 'cdecimal==2.3', 'certifi==2019.6.16', 'cffi==1.11.4', 'chardet==3.0.4', 'cld2-cffi==0.1.4', 'click==6.7', 'cloudpickle==0.5.2', 'clyent==1.2.2', 'colorama==0.3.9', 'conda-build==3.4.1', 'conda-verify==2.0.0', 'conda==4.5.4', 'configparser==3.5.0', 'contextlib2==0.5.5', 'coverage==4.5.1', 'cryptography==2.1.4', 'cycler==0.10.0', 'cymem==2.0.2', 'cython==0.27.3', 'cytoolz==0.9.0', 'dask==0.16.1', 'datashape==0.5.4', 'decorator==4.2.1', 'dill==0.2.9', 'distributed==1.20.2', 'docutils==0.14', 'ds-lime==0.1.1.27', 'eli5==0.8.1', 'entrypoints==0.2.3', 'enum34==1.1.6', 'estnltk==1.3+stacc', 'et-xmlfile==1.0.1', 'fastcache==1.0.2', 'filelock==2.0.13', 'flask-cors==3.0.3', 'flask==0.12.2', 'funcsigs==1.0.2', 'functools32==3.2.3.post2', 'funcy==1.10.1', 'future==0.16.0', 'futures==3.2.0', 'fuzzywuzzy==0.16.0', 'gast==0.2.0', 'gensim==3.7.3', 'gevent==1.2.2', 'glob2==0.6', 'gmpy2==2.0.8', 'graphframes==0.6', 'graphviz==0.10.1', 'greenlet==0.4.12', 'grin==1.2.1', 'grpcio==1.17.1', 'h5py==2.7.1', 'heapdict==1.0.0', 'html5lib==1.0.1', 'hypothesis==3.50.2', 'idna==2.8', 'imageio==2.2.0', 'imagesize==0.7.1', 'ipaddress==1.0.19', 'ipykernel==4.8.0', 'ipython-genutils==0.2.0', 'ipython==5.4.1', 'ipywidgets==7.1.1', 'isort==4.2.15', 'itsdangerous==0.24', 'jdcal==1.3', 'jedi==0.11.1', 'jinja2==2.10', 'jmespath==0.9.4', 'joblib==0.11', 'jsonschema==3.0.1', 'jupyter-client==5.2.2', 'jupyter-console==5.2.0', 'jupyter-core==4.4.0', 'jupyter==1.0.0', 'jupyterlab-launcher==0.10.2', 'jupyterlab==0.31.5', 'keras-applications==1.0.6', 'keras-preprocessing==1.0.5', 'keras==2.2.4', 'kiwisolver==1.0.1', 'langid==1.1.6', 'lazy-object-proxy==1.3.1', 'librosa==0.6.0', 'lime==0.1.1.29', 'llvmlite==0.21.0', 'locket==0.2.0', 'lorem-ipsum-generator==0.3', 'lxml==4.1.1', 'markdown==3.0.1', 'markupsafe==1.0', 'matplotlib==2.2.2', 'mccabe==0.6.1', 'mistune==0.8.3', 'mkl-fft==1.0.0', 'mkl-random==1.0.1', 'mock==2.0.0', 'mpmath==1.0.0', 'msgpack-numpy==0.4.4.2', 'msgpack-python==0.5.1', 'multipledispatch==0.4.9', 'multiprocess==0.70.6.1', 'murmurhash==1.0.2', 'navigator-updater==0.1.0', 'nbconvert==5.3.1', 'nbformat==4.4.0', 'networkx==2.1', 'nltk==3.2.5', 'nose==1.3.7', 'notebook==5.4.0', 'numba==0.36.2+0.g540650dbc.dirty', 'numexpr==2.6.4', 'numpy==1.16.4', 'numpydoc==0.7.0', 'odo==0.5.1', 'olefile==0.45.1', 'openpyxl==2.4.10', 'packaging==16.8', 'pandas==0.24.2', 'pandoc==1.0.2', 'pandocfilters==1.4.2', 'parso==0.1.1', 'partd==0.3.8', 'path.py==10.5', 'pathlib2==2.3.0', 'pathlib==1.0.1', 'patsy==0.5.0', 'pbr==5.1.1', 'pep8==1.7.1', 'pexpect==4.3.1', 'pickleshare==0.7.4', 'pillow==5.0.0', 'pip==9.0.1', 'pkginfo==1.4.1', 'plac==0.9.6', 'plotly==3.5.0', 'pluggy==0.6.0', 'ply==3.10', 'preshed==2.0.1', 'prompt-toolkit==1.0.15', 'protobuf==3.6.1', 'psutil==5.4.3', 'ptyprocess==0.5.2', 'py==1.5.2', 'pycairo==1.15.4', 'pycodestyle==2.3.1', 'pycosat==0.6.3', 'pycparser==2.18', 'pycrypto==2.6.1', 'pycurl==7.43.0.1', 'pydotplus==2.0.2', 'pyflakes==1.6.0', 'pygments==2.2.0', 'pyldavis==2.1.1', 'pylint==1.8.2', 'pyodbc==4.0.22', 'pyopenssl==17.5.0', 'pyparsing==2.2.0', 'pyrsistent==0.15.2', 'pyscaffold==2.5.8', 'pysocks==1.6.7', 'pytest-runner==4.2', 'pytest==3.3.2', 'python-crfsuite==0.9.5', 'python-dateutil==2.6.1', 'pytz==2017.3', 'pywavelets==0.5.2', 'pyyaml==3.12', 'pyzmq==16.0.3', 'qtawesome==0.4.4', 'qtconsole==4.3.1', 'qtpy==1.3.1', 'regex==2018.2.21', 'requests==2.22.0', 'resampy==0.2.0', 'retrying==1.3.3', 'rope==0.10.7', 'ruamel-yaml==0.15.35', 's3transfer==0.2.1', 'scandir==1.6', 'scikit-image==0.14.0', 'scikit-learn==0.19.1', 'scipy==1.0.1', 'seaborn==0.8.1', 'send2trash==1.4.2', 'setuptools==41.0.1', 'shap==0.26.0', 'simplegeneric==0.8.1', 'singledispatch==3.4.0.3', 'six==1.12.0', 'skater==1.1.2', 'smart-open==1.8.4', 'snorkel==0.0.12', 'snowballstemmer==1.2.1', 'sortedcollections==0.5.3', 'sortedcontainers==1.5.9', 'spacy==2.1.4', 'sphinx==1.6.6', 'sphinxcontrib-websupport==1.0.1', 'spyder==3.2.6', 'sqlalchemy==1.2.1', 'srsly==0.0.7', 'statsmodels==0.9.0', 'subprocess32==3.2.7', 'sympy==1.1.1', 'tables==3.4.2', 'tabulate==0.8.2', 'tblib==1.3.2', 'tensorboard==1.12.0', 'tensorflow==1.12.0', 'teradata==15.10.0.21', 'termcolor==1.1.0', 'terminado==0.8.1', 'testpath==0.3.1', 'textblob==0.15.1', 'thinc==7.0.4', 'toolz==0.9.0', 'torch==0.4.1', 'torchtext==0.2.3', 'torchvision==0.2.1', 'tornado==4.5.3', 'tqdm==4.32.2', 'traitlets==4.3.2', 'transliterate==1.10', 'tsfresh==0.11.1', 'typing==3.6.2', 'ujson==1.35', 'unicodecsv==0.14.1', 'unidecode==1.0.22', 'urllib3==1.25.3', 'wasabi==0.2.2', 'wcwidth==0.1.7', 'webencodings==0.5.1', 'werkzeug==0.14.1', 'wheel==0.30.0', 'widgetsnbextension==3.1.0', 'wordcloud==1.3.1', 'wrapt==1.10.11', 'xgboost==0.81', 'xlrd==1.1.0', 'xlsxwriter==1.0.2', 'xlwt==1.3.0', 'yellowbrick==0.6', 'zict==0.1.3']\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "installed_packages = pip.get_installed_distributions()\n",
    "installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\n",
    "     for i in installed_packages])\n",
    "print(installed_packages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the partitions https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t installed_packages\t installed_packages_list\t pip\t \n"
     ]
    }
   ],
   "source": [
    "#list all variables that existing in the global scope\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 2 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "#can use %%time to time the cell\n",
    "#timeit uses the Python timeit module which runs a statement 100,000 times (by default) and then provides the mean of the fastest three times.\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "#This makes it possible to go inside the function and investigate what happens there.\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pythonspot.com/save-a-dictionary-to-a-file/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#how to output infor\n",
    "start = time.time()\n",
    "#\"the code you want to test stays here\"\n",
    "pandaoutput = trx_raw.groupby(\"transtypecd\").count().sort(F.asc(\"transtypecd\")).toPandas()\n",
    "pandaoutput.to_csv(os.path.join('~/repos', 'transtype_freq' + '.csv'), encoding='utf-8')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trx_expl_pd = pd.read_csv(\"transtype_list.csv\", sep=\";\") \n",
    "#put the list into a dataframe, so that it could be merged with the other pyspark dataframe\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "StructField(\"transtypecd\", StringType(), True),\n",
    "StructField(\"class\", StringType(), True),       \n",
    "StructField(\"subclass\", StringType(), True)])\n",
    "trx_expl = sqlContext.createDataFrame(data=trx_expl_pd, schema=schema)\n",
    "trx_expl.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open(\"check_cos_ucl_output.csv\", \"w\"))\n",
    "for i in range(0,len(index)):\n",
    "    w.writerow([index[i], table_name[i], col_name[i], col_status[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing values\n",
    "# https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n",
    "# https://www.datasciencecentral.com/profiles/blogs/how-to-treat-missing-values-in-your-data-1\n",
    "# check nan in a pandas dataframe\n",
    "print(\"Total NaN in Dataframe\" , dfObj.isnull().sum().sum(), sep='\\n')\n",
    "\n",
    "print(\"***Count NaN in each column of a DataFrame***\")\n",
    "\n",
    "print(\"Nan in each columns\" , dfObj.isnull().sum(), sep='\\n')\n",
    "\n",
    "print(\"***Count NaN in each row of a DataFrame***\")\n",
    "\n",
    "for i in range(len(dfObj.index)) :\n",
    "    print(\"Nan in row \", i , \" : \" ,  dfObj.iloc[i].isnull().sum())\n",
    "# check nan in a pyspark dataframe\n",
    "# missing each row\n",
    "(df_base_full.withColumn('numNulls', sum(df_base_full[col].isNull().cast('int') for col in df_base_full.columns))\n",
    " .groupBy('numNulls').count().orderBy('numNulls').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddp_vault = sqlContext.sql(\"show tables in ddp_consumerlending_vault\")\n",
    "tables_list = ddp_vault.select('tableName').toPandas()['tableName']\n",
    "feature_tables = [t for t in tables_list if 'feature' in t]\n",
    "print(len(feature_tables),feature_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "index=[]\n",
    "table_name=[]\n",
    "col_name=[]\n",
    "col_status=[]\n",
    "for table in feature_tables:\n",
    "    print('test table: %s' %table)\n",
    "    #read the table from ddp\n",
    "    sql_str=\"select * from ddp_consumerlending_vault.\"+str(table)\n",
    "    df=sqlContext.sql(sql_str)\n",
    "    print('number of rows: %d' %df.count())\n",
    "    print('number of columns: %d' %len(df.columns))\n",
    "    for col in df.columns:\n",
    "        i=i+1\n",
    "        index.append(i)\n",
    "        table_name.append(table)\n",
    "        col_name.append(col)\n",
    "        print('test column: %s' %col)\n",
    "        if df.filter(F.col(col).isNull()).count()==0:\n",
    "            col_status.append('fully populated')\n",
    "            df.select(col).show(5,False)\n",
    "        elif (df.filter(F.col(col).isNull()).count()>0) and (df.filter(F.col(col).isNull()).count()<df.count()):\n",
    "            col_status.append('partlypopulated')\n",
    "            df.select(col).filter(F.col(col).isNotNull()).show(5,False)\n",
    "        elif df.filter(F.col(col).isNull()).count()==df.count():\n",
    "            col_status.append('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "index=[]\n",
    "table_name=[]\n",
    "col_name=[]\n",
    "col_status=[]\n",
    "for table in feature_tables:\n",
    "    print('test table: %s' %table)\n",
    "    #read the table from ddp\n",
    "    sql_str=\"select * from ddp_consumerlending_vault.\"+str(table)\n",
    "    df=sqlContext.sql(sql_str).drop('partygenid','disbursement','primaryaccountholder','default')\n",
    "    print('number of rows: %d' %df.count())\n",
    "    print('number of columns: %d' %len(df.columns))\n",
    "    for col in df.columns:\n",
    "        i=i+1\n",
    "        index.append(i)\n",
    "        table_name.append(table)\n",
    "        col_name.append(col)\n",
    "        columnList = [item[0] for item in df.dtypes if not item[1].startswith('string')]\n",
    "        print('test column: %s' %col)\n",
    "        if (df.filter(F.col(col).isNull()).count()==0):\n",
    "            if col in columnList:\n",
    "                if df.filter(F.col(col)==0).count()==0:\n",
    "                    col_status.append('100% populated')\n",
    "                else: \n",
    "                    zero_pct=np.round(df.filter(F.col(col)==0).count()*0.1/df.count()*0.1,decimals=3)\n",
    "                    col_status.append('%.4f values are 0' %zero_pct)\n",
    "            else:\n",
    "                col_status.append('100% populated')\n",
    "        elif (df.filter(F.col(col).isNull()).count()>0) and (df.filter(F.col(col).isNull()).count()<df.count()):\n",
    "            if col in columnList:\n",
    "                if df.filter(F.col(col)==0).count()==0:\n",
    "                    null_pct=np.round(df.filter(F.col(col).isNull()).count()*0.1/df.count()*0.1,decimals=3)\n",
    "                    col_status.append('%.4f missing and no zero' %null_pct)\n",
    "                else: \n",
    "                    null_pct=np.round(df.filter(F.col(col).isNull()).count()*0.1/df.count()*0.1,decimals=3)\n",
    "                    zero_pct=np.round(df.filter(F.col(col)==0).count()*0.1/df.count()*0.1,decimals=3)\n",
    "                    col_status.append('%.4f missing and %.4f zero' %(null_pct, zero_pct))\n",
    "            else:\n",
    "                null_pct=np.round(df.filter(F.col(col).isNull()).count()*0.1/df.count()*0.1,decimals=3)\n",
    "                col_status.append('%.4f missing' %null_pct)\n",
    "        elif df.filter(F.col(col).isNull()).count()==df.count():\n",
    "            col_status.append('0% populated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(index, table_name, col_name, col_status)),columns=['ID','table_name','col_name','col_status']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only \"Use This pySpark\" has the library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, NearMiss, RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work with imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "# https://www.kaggle.com/tboyle10/methods-for-dealing-with-imbalanced-data?source=post_page---------------------------\n",
    "# https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data\n",
    "# 1. oversampling the 1\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# Advantages of random over-sampling:\n",
    "\n",
    "# Unlike undersampling, this method leads to no information loss.\n",
    "\n",
    "# Disadvantages:\n",
    "\n",
    "# It increases the likelihood of overfitting since it replicates the minority class events.\n",
    "\n",
    "# 2. undersampling the 0\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# Advantages of this approach:\n",
    "\n",
    "# It can help improve the runtime of the model and solve the memory problems by reducing the number of training data samples when the training data set is enormous.\n",
    "\n",
    "# Disadvantages:\n",
    "\n",
    "# It can discard useful information about the data itself which could be necessary for building rule-based classifiers such as Random Forests.\n",
    "# The sample chosen by random undersampling may be a biased sample. And it will not be an accurate representation of the population in that case. Therefore, it can cause the classifier to perform poorly on real unseen data.\n",
    "\n",
    "\n",
    "# 3. A technique similar to upsampling is to create synthetic samples. \n",
    "# Here we will use imblearn’s SMOTE or Synthetic Minority Oversampling Technique. \n",
    "# SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n",
    "# Again, it’s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "m = SMOTE(random_state=27, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "# Advantages -\n",
    "\n",
    "# Alleviates overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances.\n",
    "# No loss of information.\n",
    "# It's simple to implement and interpret.\n",
    "\n",
    "# Disadvantages -\n",
    "\n",
    "# While generating synthetic examples, SMOTE does not take into consideration neighboring examples can be from other classes. This can increase the overlapping of classes and can introduce additional noise.\n",
    "# SMOTE is not very practical for high dimensional data.\n",
    "\n",
    "# 4. change performance matrix\n",
    "\n",
    "# 5. penalize algorithms that increase the cost of classification mistakes on the minority class e.g. svc\n",
    "# Train model\n",
    "clf_3 = SVC(kernel='linear', \n",
    "            class_weight='balanced', # penalize\n",
    "            probability=True)\n",
    "\n",
    "# 6. use tree based algorithms\n",
    "\n",
    "# 7. Use K-fold Cross-Validation in the right way\n",
    " \n",
    "# It is noteworthy that cross-validation should be applied properly while using over-sampling method to address imbalance problems.\n",
    "\n",
    "# Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won’t be an overfitting problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points\n",
    "# https://thepythonguru.com/python-string-formatting/\n",
    "# https://pyformat.info/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/\n",
    "#https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/\n",
    "#https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working with list and dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://data-flair.training/blogs/python-list-comprehension/\n",
    "#http://projectpython.net/chapter07/\n",
    "#https://opensource.com/article/18/3/loop-better-deeper-look-iteration-python\n",
    "#http://cmdlinetips.com/2018/01/5-examples-using-dict-comprehension/\n",
    "#https://www.datacamp.com/community/tutorials/python-dictionary-comprehension?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=255798340456&utm_targetid=aud-299261629574:dsa-473406574715&utm_loc_interest_ms=&utm_loc_physical_ms=1012227&gclid=CjwKCAjw6vvoBRBtEiwAZq-T1fMDu3mMD7SOWb1_z4JDDi83FoXWyzFRALGEJMpCku3WBysJRVccTBoCm4cQAvD_BwE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add new column to pandas dataframe\n",
    "#here is the simplist way to add the new column\n",
    "df['My new column'] = 'default value'\n",
    "#if you want to specify the order of the column, you can use insert\n",
    "#here, we are inserting at index 1 (so should be second col in dataframe)\n",
    "df.insert(1, 'My 2nd new column', 'default value 2')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unique value of a list\n",
    "checklist=[1,1,2,2,3,3,4,4]\n",
    "mylist = list(set(checklist))\n",
    "mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work with partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_partition(df):\n",
    "\n",
    "    print(\"Num partition: {0}\".format(df.rdd.getNumPartitions()))\n",
    "\n",
    "     \n",
    "\n",
    "    def count_partition(index, iterator):\n",
    "\n",
    "        yield (index, len(list(iterator)))\n",
    "\n",
    "         \n",
    "\n",
    "    data = (df.rdd.mapPartitionsWithIndex(count_partition, True).collect())\n",
    "\n",
    "     \n",
    "\n",
    "    for index, count in data:\n",
    "\n",
    "        print(\"partition {0:2d}: {1} bytes\".format(index, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages: pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cache a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to cache a table\n",
    "start = time.time()\n",
    "trx.cache().count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to save a table to hive 1 (make sure the dataframe has no duplicate column)\n",
    "trx.write.format('orc').saveAsTable('ddp_steer.p901cyo_trx', mode = 'overwrite')\n",
    "#how to save a table to hive 2\n",
    "trx.createOrReplaceTempView('trx_temp')\n",
    "spark.sql('drop table if exists ddp_steer.p901cyo_trx')\n",
    "spark.sql('create table ddp_steer.p901cyo_trx as select * from trx_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop cache method 1\n",
    "df1.unpersist()\n",
    "# drop cache method 2\n",
    "spark.catalog.uncacheTable(tableName)\n",
    "spark.catalog.clearCache()\n",
    "#trx_sample.is_cached\n",
    "#  useDisk, useMemory, useOffHeap, deserialized, replication=1\n",
    "#trx_sample.storageLevel\n",
    "#spark.catalog.clearCache()\n",
    "#spark.catalog.uncacheTable(tableName='trx_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'1'),\n",
       " (u'spark.driver.memory', u'2G'),\n",
       " (u'spark.history.kerberos.principal', u'spark-SWEDBANKSEHA2@FSPA.MYNTET.SE'),\n",
       " (u'spark.history.ui.port', u'18081'),\n",
       " (u'spark.driver.extraLibraryPath',\n",
       "  u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " (u'spark.driver.appUIAddress', u'http://39.7.48.31:4051'),\n",
       " (u'spark.dynamicAllocation.maxExecutors', u'15'),\n",
       " (u'spark.ui.killEnabled', u'true'),\n",
       " (u'spark.history.kerberos.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.executorIdleTimeout', u'60'),\n",
       " (u'spark.executor.extraLibraryPath',\n",
       "  u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " (u'spark.yarn.historyServer.address', u'sb-hdp-m3.fspa.myntet.se:18081'),\n",
       " (u'spark.history.provider',\n",
       "  u'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.dynamicAllocation.initialExecutors', u'1'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.app.id', u'application_1560417165231_2526'),\n",
       " (u'spark.eventLog.dir', u'hdfs:///spark2-history/'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip:/usr/hdp/current/spark2-client/python/<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.4-src.zip'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'sb-hdp-m2.fspa.myntet.se,sb-hdp-m3.fspa.myntet.se'),\n",
       " (u'spark.driver.maxResultSize', u'2G'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.driver.port', u'44381'),\n",
       " (u'spark.yarn.queue', u'default'),\n",
       " (u'spark.history.kerberos.keytab',\n",
       "  u'/etc/security/keytabs/spark.headless.keytab'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.port.maxRetries', u'50'),\n",
       " (u'spark.sql.hive.convertMetastoreOrc', u'false'),\n",
       " (u'spark.app.name', u'pyspark-shell'),\n",
       " (u'spark.history.fs.logDirectory', u'hdfs:///spark2-history/'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.host', u'39.7.48.31'),\n",
       " (u'spark.executor.instances', u'1'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'https://sb-hdp-m2.fspa.myntet.se:8090/proxy/application_1560417165231_2526,https://sb-hdp-m3.fspa.myntet.se:8090/proxy/application_1560417165231_2526'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.get('spark.executor.memory')\n",
    "sc._conf.get('spark.driver.memory')\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #可以用sql选数据了\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext #udf\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf # udf takes two iput, function and return type\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "#https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 1226\n"
     ]
    }
   ],
   "source": [
    "#transaction data is updated to 2018May #and transactiondate='2018-05-01'\n",
    "df=spark.sql(\"select * from ddp_central.depo_transactionext where transactionmonth='201805'\")\n",
    "print(\"Partitions: \" + str(df.rdd.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- primaryaccountholder: binary (nullable = true)\n",
      " |-- agreementgenid: integer (nullable = true)\n",
      " |-- productgenid: binary (nullable = true)\n",
      " |-- transtypecd: string (nullable = true)\n",
      " |-- transactiondate: date (nullable = true)\n",
      " |-- accountingtypecd: string (nullable = true)\n",
      " |-- channeltypecd: string (nullable = true)\n",
      " |-- amount: decimal(18,2) (nullable = true)\n",
      " |-- bankid: integer (nullable = true)\n",
      " |-- secaccounttypecd: string (nullable = true)\n",
      " |-- secclearingnum: decimal(5,0) (nullable = true)\n",
      " |-- secaccountnum: string (nullable = true)\n",
      " |-- secagreementgenid: integer (nullable = true)\n",
      " |-- secpartygenid: binary (nullable = true)\n",
      " |-- secproductgenid: binary (nullable = true)\n",
      " |-- channelgenid: integer (nullable = true)\n",
      " |-- transactiongenid: integer (nullable = true)\n",
      " |-- systemsourcecd: string (nullable = true)\n",
      " |-- identificationmethodcd: string (nullable = true)\n",
      " |-- paymentroutinecd: string (nullable = true)\n",
      " |-- secbic: string (nullable = true)\n",
      " |-- clearingnum: decimal(5,0) (nullable = true)\n",
      " |-- cardtransactionchannelcd: string (nullable = true)\n",
      " |-- transactioninfo: string (nullable = true)\n",
      " |-- kanalgenid: string (nullable = true)\n",
      " |-- transactionmonth: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-----------+---------------+----------------+-------------+-------+------+----------------+--------------+-------------+-----------------+--------------------+--------------------+------------+----------------+--------------+----------------------+----------------+------+-----------+------------------------+---------------+--------------------+----------------+\n",
      "|primaryaccountholder|agreementgenid|        productgenid|transtypecd|transactiondate|accountingtypecd|channeltypecd| amount|bankid|secaccounttypecd|secclearingnum|secaccountnum|secagreementgenid|       secpartygenid|     secproductgenid|channelgenid|transactiongenid|systemsourcecd|identificationmethodcd|paymentroutinecd|secbic|clearingnum|cardtransactionchannelcd|transactioninfo|          kanalgenid|transactionmonth|\n",
      "+--------------------+--------------+--------------------+-----------+---------------+----------------+-------------+-------+------+----------------+--------------+-------------+-----------------+--------------------+--------------------+------------+----------------+--------------+----------------------+----------------+------+-----------+------------------------+---------------+--------------------+----------------+\n",
      "|[19 97 06 02 16 4...|     167410368|[20 05 05 01 00 0...|   TOB261  |     2018-05-27|             UT |          POS|-455.71|  8999|            null|          null|         null|             null|[19 97 06 02 16 4...|                  []|         359|       268346724|           GRK|                  null|            null|  null|      88500|                     KSV|          20443|POSICA SUPERMARKE...|          201805|\n",
      "|[19 97 06 02 16 4...|     198255547|[20 05 12 01 00 2...|   TOB552  |     2018-05-26|             IN |          INT|3000.00|  8999|            null|          null|         null|        169576175|[05 06 20 09 39 3...|[20 05 12 01 00 2...|          39|       272480156|           GRK|                  null|            null|  null|      84244|                    null|842449042619115|          INT8424969|          201805|\n",
      "+--------------------+--------------+--------------------+-----------+---------------+----------------+-------------+-------+------+----------------+--------------+-------------+-----------------+--------------------+--------------------+------------+----------------+--------------+----------------------+----------------+------+-----------+------------------------+---------------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily_count=df.groupBy(\"transactiondate\").count().sort('transactiondate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(daily_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['transactiondate', 'count'], 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_count.columns, len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cn in df_base.drop(\"partygenid\", \"Customer_Group\", \"target\", \"reportdate\").columns:\n",
    "    df_base = df_base.withColumnRenamed(str(cn), \"base_\"+str(cn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+----------------+-------------+-------------------+---------+----------------+------------------+-------------------+--------------------+------------------+--------------------+--------------+----------------------+--------------------+-----------+-----------------+------------------------+----------------+----------+----------------+\n",
      "|summary|      agreementgenid|transtypecd|accountingtypecd|channeltypecd|             amount|   bankid|secaccounttypecd|    secclearingnum|      secaccountnum|   secagreementgenid|      channelgenid|    transactiongenid|systemsourcecd|identificationmethodcd|    paymentroutinecd|     secbic|      clearingnum|cardtransactionchannelcd| transactioninfo|kanalgenid|transactionmonth|\n",
      "+-------+--------------------+-----------+----------------+-------------+-------------------+---------+----------------+------------------+-------------------+--------------------+------------------+--------------------+--------------+----------------------+--------------------+-----------+-----------------+------------------------+----------------+----------+----------------+\n",
      "|  count|           175977105|  175977105|       175977105|    175977105|          175977105|175977105|        37690631|          14813577|           37690631|            25402968|         175977105|           175977105|     175977105|              22877051|            22877051|   22877051|        175977105|                85315641|       175977105| 175977105|       175977105|\n",
      "|   mean|1.9390818212422138E8|       null|            null|         null|         383.307197|   8999.0|            null|          381.3961|7.152532765224807E7|2.0076676556257698E8|243.35413511320124|1.5773174838534877E8|          null|                  null|                11.0|       null|       86691.0095|                    null|        Infinity|      null|        201805.0|\n",
      "| stddev|4.7966522528110676E7|       null|            null|         null|5.737285928742055E7|      0.0|            null|1576.5013829971158|5.191088528460941E8| 5.038650462623681E7|134.22236513814786|1.0635436969740449E8|          null|                  null|1.729535847142679...|       null|7367.782471092276|                    null|             NaN|      null|             0.0|\n",
      "|    min|                   0|   ATL0000 |             IN |          ATM|   -339987816666.67|     8999|             BG |                 0|         0000000018|           147671321|                 1|              616799|           ATL|                   MOB|                 011|AABASESS   |                0|                     AAB|                |ATM8032002|          201805|\n",
      "|    max|           319859671|   TOBSPOT |             UT |          TSB|    385000000000.00|     8999|             PG |              9924|              XXXXX|           319492296|               418|          1917697340|           GRK|                   MOB|                 011|SWEDSESS   |            99000|                     UUT|Šårs bilförsäkri|    TSB970|          201805|\n",
      "+-------+--------------------+-----------+----------------+-------------+-------------------+---------+----------------+------------------+-------------------+--------------------+------------------+--------------------+--------------+----------------------+--------------------+-----------+-----------------+------------------------+----------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|             amount|\n",
      "+-------+-------------------+\n",
      "|  count|          175977105|\n",
      "|   mean|         383.307197|\n",
      "| stddev|5.737285928742056E7|\n",
      "|    min|   -339987816666.67|\n",
      "|    max|    385000000000.00|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.agg(F.max(df.amount)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.agg({\"amount\":\"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|transactioninfo|\n",
      "+---------------+\n",
      "|          20443|\n",
      "|842449042619115|\n",
      "|832799143329580|\n",
      "|         150143|\n",
      "|           7228|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('transactioninfo').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('transactioninfo').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.crosstab('cardtransactionchannelcd','identificationmethodcd').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n and amt of swish trx in a time window\n",
    "    df_swish_amt=df_swish.groupby('primaryaccountholder','disbursement_month').pivot('swish_prod_type').agg(\n",
    "        F.sum(F.when(F.trim(F.col('accountingtypecd')) == 'IN',1).otherwise(0)).alias('n_swish_in'),\n",
    "        F.sum(F.when(F.trim(F.col('accountingtypecd')) == 'UT',1).otherwise(0)).alias('n_swish_ut'),\n",
    "        F.sum(F.when(F.trim(F.col('accountingtypecd')) == 'IN',F.col('amount')).otherwise(0)).alias('amt_swish_in'),\n",
    "        F.sum(F.when(F.trim(F.col('accountingtypecd')) == 'UT',F.col('amount')).otherwise(0)).alias('amt_swish_ut')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take sample with replacement=T or F, fraction=0.2, seed for the result\n",
    "s1=df.sample(False,0.2,123)\n",
    "s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to take a sample\n",
    "trx_sample=trx.sample(withReplacement=False, fraction=0.1, seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define windowspec\n",
    "trx_window = Window.partitionBy(\"category\",\"provider\",\"secaccountnum\").orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactioninfo=Debt_toother.groupby('category','provider','secaccountnum','transactioninfo').count().orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use dense_rank\n",
    "check=transactioninfo.withColumn(\"rank\", F.dense_rank().over(trx_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use row_number\n",
    "check=transactioninfo.withColumn(\"rank\", F.row_number().over(trx_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use lag\n",
    "TOB_transfer_recur.withColumn(\"lag_id\", F.lag(TOB_transfer_recur.partygenid).over(trx_window))\\\n",
    "                  .withColumn(\"lag_amt\",F.lag(TOB_transfer_recur.amount).over(trx_window))\\\n",
    "                  .withColumn(\"lag_date\", F.lag(TOB_transfer_recur.day).over(trx_window_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create id\n",
    ".withColumn(\"ID\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when().otherwise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transJoined = trans_raw.select(*[c for c in trans_raw.columns if c != 'productgenid'])\\\n",
    "        .join(broadcast(amtrLookup), ['transtypecd'], 'left')\\\n",
    "        .withColumn('accountingtypecd', trim(trans_raw.accountingtypecd))\\\n",
    "        .withColumn('amount', trans_raw.amount.cast(DecimalType(16,2)))\\\n",
    "        .withColumn('toSelf', when(trans_raw.primaryaccountholder == trans_raw.secpartygenid, 1).otherwise(0))\\\n",
    "        .withColumn('toOther', when(trans_raw.primaryaccountholder != trans_raw.secpartygenid, 1).otherwise(0))\\\n",
    "        .withColumn('count', lit(1))\\\n",
    "        .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([('abcd',)], ['s',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.select(instr(test.s, 'b').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for, reduce and list comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for loops seem to yield the most readable code. \n",
    "#List comprehensions can be used for operations that are performed on all columns of a DataFrame, \n",
    "#but should be avoided for operations performed on a subset of the columns. \n",
    "#The reduce code is pretty clean too, so that’s also a viable alternative. \n",
    "#It’s best to write functions that operate on a single column and \n",
    "#wrap the iterator in a separate DataFrame transformation so the code can easily be applied to multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lowercase all columns with a list comprehension\n",
    "   \n",
    "actual_df = source_df.select(\n",
    "    *[lower(col(col_name)).name(col_name) for col_name in source_df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lowercase all columns with reduce\n",
    "#  import the reduce function from functools and use it to lowercase all the columns in a DataFrame.\n",
    "actual_df = (reduce(\n",
    "    lambda memo_df, col_name: memo_df.withColumn(col_name, lower(col(col_name))),\n",
    "    source_df.columns,\n",
    "    source_df\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lowercase all columns with a for loop\n",
    "actual_df = source_df\n",
    "\n",
    "for col_name in actual_df.columns:\n",
    "    actual_df = actual_df.withColumn(col_name, lower(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performing operations on a subset of the DataFrame columns\n",
    " #define a remove_some_chars function that removes all exclamation points and question marks from a column.\n",
    "def remove_some_chars(col_name):\n",
    "    removed_chars = (\"!\", \"?\")\n",
    "    regexp = \"|\".join('\\{0}'.format(i) for i in removed_chars)\n",
    "    return regexp_replace(col_name, regexp, \"\")\n",
    "source_df.select(\n",
    "    *[remove_some_chars(col_name).name(col_name) if col_name in [\"sport\", \"team\"] else col_name for col_name in source_df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp table\n",
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "query = '''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n",
    "afj.explain() #this helps to understand the execution plan and fune performance of spark jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "#https://spark.apache.org/docs/2.2.0/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method 1\n",
    "trainraw_clean.withColumn('transactioninfo_split',F.split(trainraw['transactioninfo_cleaned'],' '))\\\n",
    ".withColumn('item1',F.col('transactioninfo_split').getItem(0))\\\n",
    ".select(\"*\", F.posexplode(F.split(\"transactioninfo_cleaned\", \" \")).alias(\"pos\", \"val\"))\\\n",
    ".drop(\"val\")\\\n",
    ".select(\n",
    "        \"*\",\n",
    "        F.concat(F.lit(\"transactioninfo_cleaned\"),F.col(\"pos\").cast(\"string\")).alias(\"name\"),\n",
    "        F.expr(\"transactioninfo_split[pos]\").alias(\"val\")\n",
    "    )\\\n",
    ".groupBy(\"transactioninfo_cleaned\").pivot(\"name\").agg(F.first(\"val\"))\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method 2 use TF-IDF term frequency-inverse document frequency to extract feature\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "#tokenization\n",
    "tokenizer=Tokenizer(inputCol=\"transactioninfo_cleaned\", outputCol=\"words\")\n",
    "wordsData=tokenizer.transform(trainraw_clean)\n",
    "wordsData.show(5)\n",
    "print(wordsData.count())\n",
    "\n",
    "#count features\n",
    "# numFeatures is to limit the Feature numbers, don't have to specify\n",
    "# hashingTF=HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\", numFeatures=1500)\n",
    "hashingTF=HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\")\n",
    "featurizedData=hashingTF.transform(wordsData)\n",
    "featurizedData.show(50,truncate=False)\n",
    "\n",
    "# Intuitively, it down-weights columns which appear frequently in a corpus\n",
    "idf=IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel=idf.fit(featurizedData)\n",
    "rescaledData=idfModel.transform(featurizedData)\n",
    "rescaledData.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method 3 use regex\n",
    "\n",
    "#use regextokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"transactioninfo_cleaned\", outputCol=\"words\", minTokenLength=3, pattern=(\"\\\\W\"))\n",
    "countTokens=F.udf(lambda words: len(words), IntegerType())\n",
    "regexTokenized = regexTokenizer.transform(trainraw_clean)\n",
    "regexTokenized.select(\"transactioninfo_cleaned\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(F.col(\"words\"))).show(truncate=False)\n",
    "    \n",
    "hashingTF=HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\")\n",
    "featurizedData=hashingTF.transform(regexTokenized)\n",
    "featurizedData.filter(F.col('target')==1).show(50,truncate=False)\n",
    "\n",
    "idf=IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel=idf.fit(featurizedData)\n",
    "rescaledData=idfModel.transform(featurizedData)\n",
    "rescaledData.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use countvectorizer to count. it is like hashingTF and featurizeddata\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "# use wordsData from cell 27\n",
    "#if don't specify vocabSize, if will take all vocab\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=1500, minDF=2.0) \n",
    "model = cv.fit(wordsData)\n",
    "\n",
    "result = model.transform(wordsData)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use word2vec https://github.com/apache/spark/blob/master/examples/src/main/python/ml/word2vec_example.py\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "# use wordsData from cell 27\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"result\")\n",
    "model = word2Vec.fit(wordsData)\n",
    "\n",
    "result = model.transform(wordsData)\n",
    "# the following part doesn't run\n",
    "# for row in result.collect():\n",
    "#     text, vector = row\n",
    "#     print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extract features with grouping words together, e.g. n=2, then takes 2 words sequentially\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordsData)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Stop Words Remover https://github.com/apache/spark/blob/master/examples/src/main/python/ml/stopwords_remover_example.py\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# use wordsData from cell 27\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.transform(wordsData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html\n",
    "lines = sc.textFile(\"greetings.txt\")\n",
    "lines.map(lambda line: line.split()).collect() #map operates within each element\n",
    "lines.flatMap(lambda line: line.split()).collect() #flatmap flats all the elements into one element\n",
    "\n",
    "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()\n",
    "sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1=\"To be or not to be\"\n",
    "len(text1)\n",
    "text2=text1.split(' ')\n",
    "[w for w in text2 if len(w)>3]\n",
    "[w for w in text2 if w.istitle()]#check if the first letter is captalized \n",
    "[w for w in text 2 if w.endswith('s')]\n",
    "#s.startswith(t)\n",
    "#t in s\n",
    "#s.isupper(); s.islower(); s.istitle(); s.isalpha(); s.isdigit(), s.isalnum()\n",
    "len(set(text2)) #to find the unique words\n",
    "len(set([w.lower() for w in text2]))\n",
    "\n",
    "#text=s.split('t') and t.join(text)=s\n",
    "#s.splitlines()\n",
    "#s.strip() take out spaces from the front and the end\n",
    "#s.rstrip() take out spaces from the end\n",
    "#s.find(t); s.rfind(t)\n",
    "#s.replace(u,v)\n",
    "\n",
    "#e.g. find hashtags\n",
    "tweet = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n",
    "c=[w for w in tweet.split(' ') if w.startswith('#')]\n",
    "\n",
    "import re\n",
    "#e.g. find callouts\n",
    "[w for w in text if re.search('@[A-Za-z0-9_]+',w)]\n",
    "\n",
    "#.:matches a single character\n",
    "#^:start of a string\n",
    "#$:end of a string\n",
    "#[]: matches one of the set of characters within []\n",
    "#[a-z]: matches one of the range of characters a,b,...,z\n",
    "#[^abc]: matches a character that is not a,b,or c\n",
    "#a|b: marches either a or b, where a and b are strings\n",
    "#(): Scoping for operators\n",
    "#\\:escape character for special characters(\\t,\\n,\\b)\n",
    "#\\b: matches word boundry\n",
    "#\\d: any digit, equivalent to [0-9]\n",
    "#\\D: any non-digit, equivalent to [^0-9]\n",
    "#\\s: any whitespace, equivalent to [ \\t\\n\\r\\f\\v]\n",
    "#\\S:any non-whitespace, equivalent to [^ \\t\\n\\r\\f\\v]\n",
    "#\\w: alphanumeric character, equivalent to [a-zA-Z0-0_]\n",
    "#\\W: non-alphanumeric, equivalent to [^a-zA-Z0-9_]\n",
    "#*:,matches 0 or more\n",
    "#+:matches one or more\n",
    "#?:matches zero or one\n",
    "#{n}:exactly n times\n",
    "#{n,}:at least n times\n",
    "#{,n}: at most n times\n",
    "#{m,n}: at least m and at most n times\n",
    "\n",
    "#e.g. In this case, the input is \"\\s+\": \"\\s\" is a special character that matches any whitespace (space, tab, newline, etc.), \n",
    "#and the \"+\" is a character that indicates one or more of the entity preceding it. \n",
    "#Thus, the regular expression matches any substring consisting of one or more spaces.\n",
    "#e.g. email = re.compile('\\w+@\\w+\\.[a-z]{3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regular expression for dates:\n",
    "dataStr='23-10-2002\\n23/10/2002\\n23/10/02\\n10/23/2002\\n23 Oct 2002\\n23 October 2002\\nOct 23, 2002\\nOctober 23, 2002\\n'\n",
    "re.findall(r'\\d{2}[/-]\\d{2}[/-]\\d{4}'),dateStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text data manipulation\n",
    "fox=\"tHe qUICk fOx\"\n",
    "fox.upper()\n",
    "fox.lower()\n",
    "fox.title()\n",
    "fox.capitalize()\n",
    "fox.swapcase()\n",
    "#remove whitespace from the beginning and end of the line\n",
    "panda='   this is the content   '\n",
    "panda.strip()\n",
    "panda.rstrip()\n",
    "panda.lstrip()\n",
    "#remove desired characters to be removed\n",
    "num=\"00000000435\"\n",
    "num.strip('0')\n",
    "#add white space\n",
    "fox.center(30)\n",
    "fox.ljust(30)\n",
    "fox.rjust(30)\n",
    "'435'.rjust(10,'0')\n",
    "'435'.zfill(10)\n",
    "#find occurrences of a certain character in a string find()/rfind(), index/rindex() and replace()\n",
    "#The only difference between find() and index() is their behavior when the search string is not found; \n",
    "#find() returns -1, while index() raises a ValueError\n",
    "\n",
    "fox.endswith('dog') -->true or false\n",
    "fox.startswith('fox')-->true or false\n",
    "fox.replace('a','*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting and partitioning strings\n",
    "line='the quick brown fox jumped over a lazy dog'\n",
    "line.partition('fox') --> with partition at fox and partition line into three parts\n",
    "#rpartition() just searches from the right\n",
    "line.split()--> default is to split on any whitespace\n",
    "line.splitlines()\n",
    "#if you want to undo a split you can use join\n",
    "'--'.join(['1','2','3'])\n",
    "\"\\n\".join(['ya','nej','jo']) -->\"\\n\" means new line, this is to join the lines together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format string\n",
    "pi=3.14159\n",
    "str(pi)\n",
    "\"the value of pi is\" + str(pi)\n",
    "\"the value of pi is {}\".format(pi)\n",
    "#Inside the {} marker you can also include information on exactly what you would like to appear there. \n",
    "#If you include a number, it will refer to the index of the argument to insert\n",
    "\"\"\"First letter: {0}. Last letter: {1}.\"\"\".format('A', 'Z')\n",
    "\"\"\"First letter: {first}. Last letter: {last}.\"\"\".format(last='Z', first='A')\n",
    "\"pi = {0:.3f}\".format(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line='the quick brown fox jumped over a lazy dog'\n",
    "regex=re.compile('\\s')\n",
    "regex.split(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in [\"     \", \"abc  \", \"  abc\"]:\n",
    "    if regex.match(s):\n",
    "        print(repr(s), \"matches\")\n",
    "    else:\n",
    "        print(repr(s), \"does not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split, into multiple rows, then transpose to multiple columns\n",
    "df.select(\n",
    "        \"num\",\n",
    "        f.split(\"letters\", \", \").alias(\"letters\"),\n",
    "        f.posexplode(f.split(\"letters\", \", \")).alias(\"pos\", \"val\")\n",
    "    )\\\n",
    "    .drop(\"val\")\\\n",
    "    .select(\n",
    "        \"num\",\n",
    "        f.concat(f.lit(\"letter\"),f.col(\"pos\").cast(\"string\")).alias(\"name\"),\n",
    "        f.expr(\"letters[pos]\").alias(\"val\")\n",
    "    )\\\n",
    "    .groupBy(\"num\").pivot(\"name\").agg(f.first(\"val\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concat, concat_ws, collect_set, collect_list, explode, split, explode_outer, expr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "levenshtein1 = (levenshtein(df1['single_word'], df2['single_word_comp']) < 1)\n",
    "levenshtein2 = (levenshtein(df1['single_word'], df2['single_word_comp']) < 2)\n",
    "levenshtein3 = (levenshtein(df1['single_word'], df2['single_word_comp']) < 3)\n",
    "\n",
    "join_condition = ((F.when(length(df1['single_word']) <= 2, levenshtein1)) |\n",
    "                (F.when(length(df1['single_word']) == 3, levenshtein2)) |\n",
    "                (F.when(length(df1['single_word']) == 4, levenshtein2)) |\n",
    "                (F.when(length(df1['single_word']) > 4, levenshtein3)))\n",
    "\n",
    "df_levenshtein = (df1.join(df2, join_condition)\n",
    "    .withColumn('levenshtein', levenshtein(df1['single_word'], df2['single_word_comp']))\n",
    "    .sort(desc('kundnumr'),desc('kundnumr_comp'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data cleaning function\n",
    "\n",
    "expr = '[^A-Za-zÀ-ÿ0-9]+'\n",
    "\n",
    "def datacleaning(selected_column, clean_column, inputdata):\n",
    "    outputdata = (inputdata\n",
    "        .withColumn(clean_column, F.lower(F.col(selected_column)))\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column), expr, \" \") )\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column),\"å\",\"ao\"))\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column),\"ä\",\"ae\"))\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column),\"ö\",\"oe\"))\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column), '\\\\b(\\d{2,})\\\\b', \"\") )\n",
    "        .withColumn(clean_column,\n",
    "        F.regexp_replace(F.col(clean_column), '\\s+', \" \") )\n",
    "        .withColumn(clean_column, F.trim(F.col(clean_column)))\n",
    "        )\n",
    "    return outputdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# wordcloudpic without removing stopwords\n",
    "def wordcloudpic(inputdata, inputCol):\n",
    "    regexTokenizer = RegexTokenizer(inputCol=inputCol, outputCol=\"words\", minTokenLength=2, pattern=(\"\\\\W\"))\n",
    "    countTokens=F.udf(lambda words: len(words), IntegerType())\n",
    "    regexTokenized = regexTokenizer.transform(inputdata)   \n",
    "    words = (regexTokenized\n",
    "         .select(F.posexplode('words').alias('l1','words'))\n",
    "        .drop('l1').groupby('words')\n",
    "         .agg(F.count('words').astype('float').alias('counts'))\n",
    "         .orderBy(F.desc('counts')).toPandas())\n",
    "\n",
    "    d={}\n",
    "    for a, x in words.values:\n",
    "        d[a]=int(x)\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width = 900,\n",
    "        height = 500,\n",
    "        background_color = 'white').generate_from_frequencies(frequencies=d)\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordcloudpic with removing stopwords\n",
    "\n",
    "def wordcloudpic2(inputdata, inputCol):\n",
    "    regexTokenizer = RegexTokenizer(inputCol=inputCol, outputCol=\"words\", minTokenLength=2, pattern=(\"\\\\W\"))\n",
    "    countTokens=F.udf(lambda words: len(words), IntegerType())\n",
    "    regexTokenized = regexTokenizer.transform(inputdata)\n",
    "    stopwordsRemover=StopWordsRemover(inputCol=\"words\",outputCol=\"filtered\").setStopWords(add_words)\n",
    "    stopwordscleaned=stopwordsRemover.transform(regexTokenized)\n",
    "    \n",
    "    filtered = (stopwordscleaned\n",
    "         .select(F.posexplode('filtered').alias('l1','filtered'))\n",
    "        .drop('l1').groupby('filtered')\n",
    "         .agg(F.count('filtered').astype('float').alias('counts'))\n",
    "         .orderBy(F.desc('counts')).toPandas())\n",
    "\n",
    "    d={}\n",
    "    for a, x in filtered.values:\n",
    "        d[a]=int(x)\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width = 900,\n",
    "        height = 500,\n",
    "        background_color = 'white').generate_from_frequencies(frequencies=d)\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstelement=F.udf(lambda v:float(v[0]),FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(3,2,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# web scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.bloomberg.com/quote/'\n",
    "\n",
    "# url_1 = base_url + 'BOL:SS'\n",
    "\n",
    "search_ls = ['BOL:SS', 'EVO:SS', 'HMS:SS', 'HOLMB:SS', 'KAHL:SS', 'NOBINA:SS', 'SAND:SS', 'SKISB:SS', 'VOLVB:SS']\n",
    "# 初始化一个列表来保存所有的帖子信息：\n",
    "comments = []\n",
    "\n",
    "# 首先我们写好抓取网页的函数\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout=30)\n",
    "        # r.raise_for_status()\n",
    "        #这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用：\n",
    "        # r.endcodding = r.apparent_endconding()\n",
    "        # r.encoding='utf-8'\n",
    "        return r.text\n",
    "    except:\n",
    "        return \" ERROR \"\n",
    "\n",
    "def get_content(url, share_ls):\n",
    "    '''\n",
    "    analyze the webpage\n",
    "    '''\n",
    "\n",
    "    # initialize a dict for storing the results\n",
    "    comments = []\n",
    "\n",
    "    for share_t in share_ls:\n",
    "        comment = {}\n",
    "\n",
    "        url_sub = base_url + share_t\n",
    "\n",
    "        print(url_sub)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            # download the webpage\n",
    "            html = get_html(url_sub)\n",
    "\n",
    "            # analyze it use BS4\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            comment['company_name'] = soup.find('h1', attrs={'class': 'companyName__99a4824b'}).text.strip()\n",
    "            comment['latest_price'] = soup.find('span', attrs={'class': 'priceText__1853e8a5'}).text.strip()\n",
    "\n",
    "            comments.append(comment)\n",
    "\n",
    "            # print(comments)\n",
    "\n",
    "        except:\n",
    "            print('fail')\n",
    "\n",
    "    return comments\n",
    "\n",
    "content = get_content(base_url, search_ls)\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/R/column_datetime_functions.html\n",
    "#https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\n",
    "#https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\n",
    "byDayOfWeekDF = (pageviewsDF\n",
    "  .groupBy( date_format( col(\"capturedAt\"), \"E\") )         # format as Mon, Tue and then aggregate\n",
    "  .sum()                                                   # produce the sum of all records to get the dataframe\n",
    "  .select( col(\"date_format(capturedAt, E)\").alias(\"dow\"), # rename to \"dow\"\n",
    "           col(\"sum(requests)\").alias(\"total\"))            # rename to \"total\"\n",
    "  .orderBy( col(\"dow\") )                                   # sort by \"dow\" MTWTFSS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add date\n",
    "#https://stackoverflow.com/questions/45612208/add-months-to-date-column-in-spark-dataframe\n",
    ".withColumn('perf_window',F.add_months(F.col('disbursement_month'), -1))\n",
    "#add date in sql\n",
    "#http://sqlandhadoop.com/hive-date-functions-all-possible-date-operations/\n",
    "date_add(a.disbursement_month,-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql('b.transactiondate>=add_months(a.disbursement_month,-6)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not a real date format, can't be used to calculate monthdiff later.\n",
    "spark.sql('date_format(trx.transactiondate, \"YYYYMM\") as transaction_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ".withColumn('monthdiff',F.months_between(F.col(\"transactionmonth\"),F.col('disbursement_month')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get first day of a month\n",
    ".withColumn('transactionmonth',F.trunc(\"transactiondate\", \"month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last day of a month\n",
    ".withColumn('perf_month',F.last_day(F.add_months(F.col('disbursement_month'), -6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ".withColumn('transaction_month', \n",
    "        F.concat_ws(\"-\",F.col('transactionmonth').substr(0,4),F.col('transactionmonth').substr(5,6))) \\\n",
    ".withColumn('monthdiff',F.months_between(F.col('transaction_month'),F.col('disbursement_month')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#undirected network\n",
    "G=nx.Graph()\n",
    "G.add_edge('A','B')\n",
    "#directed network\n",
    "G=nx.DiGraph()\n",
    "G.add_edge('B','A')\n",
    "G.add_edge('B','C')\n",
    "#weighted network\n",
    "G=nx.Graph()\n",
    "G.add_edge('A','B',weight=6)\n",
    " #can give the edge any attributes\n",
    "G.add_edge('A','B',weight=6, relation='friends')\n",
    " #multigraphs: multiple edges can connect the same pair of nodes(parallel edges)\n",
    "G=nx.MultiGraph()\n",
    "G.add_edge('A','B',relation='friends')\n",
    "G.add_edge('A','B',relation='neighbor')\n",
    "#access the edges\n",
    " #list all the edges\n",
    "G.edges()\n",
    " #list all the edges with attributes\n",
    "G.edges(data=True)\n",
    " #list all the edges with specific attribute\n",
    "G.edges(data='relation')\n",
    " #access a specific edge\n",
    "G.edge['A']['B']\n",
    "G.edge['A']['B']['weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/feature-engineering-in-python-part-i-the-most-powerful-way-of-dealing-with-data-8e2447e7c69e\n",
    "#https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Use This pySpark (Spark 2.2)",
   "language": "python",
   "name": "usethis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
