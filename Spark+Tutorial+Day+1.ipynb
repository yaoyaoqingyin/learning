{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "254d1d36-09ac-426d-9782-6ce50275cb72"
    }
   },
   "source": [
    "## 1. Some Basics\n",
    "\n",
    "**This is your jupyter notebook. You can use this to connect to Spark on DDP and run your code. **\n",
    "\n",
    "Notebooks are made up of cells, and you can run 1 cell at a time. \n",
    "\n",
    "To view options on Jupyter notebook press Help>Keyboard Shortcuts, some useful ones:\n",
    "1. Ctrl + Enter = Run Cell\n",
    "2. Esc + A = Add Cell Above\n",
    "3. Esc + B = Add Cell Below\n",
    "4. Esc + L = Show Line numbers\n",
    "5. Ctrl + S = Save Notenbook\n",
    "\n",
    "Tips!\n",
    "* Use InternetExplorer or FireFox, Chrome can sometimes be slow.\n",
    "* Dont Open too many notebooks at the same time\n",
    "* Remember to save as frequently as you can! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 This is some tips on how to edit text in Markdowns\n",
    "\n",
    "\n",
    "# This is a level 1 heading\n",
    "## This is a level 2 heading\n",
    "This is some plain text that forms a paragraph.\n",
    "Add emphasis via **bold** and __bold__, or *italic* and _italic_.\n",
    "\n",
    "Paragraphs must be separated by an empty line.\n",
    "\n",
    "* Sometimes we want to include lists.\n",
    " * Which can be indented.\n",
    "\n",
    "1. Lists can also be numbered.\n",
    "2. For ordered lists.\n",
    "\n",
    "[It is possible to include hyperlinks](https://www.example.com)\n",
    "\n",
    "Inline code uses single backticks: `foo()`, and code blocks use triple backticks:\n",
    "\n",
    "```\n",
    "bar()\n",
    "```\n",
    "\n",
    "Or can be indented by 4 spaces:\n",
    "\n",
    "    foo()\n",
    "\n",
    "And finally, adding images is easy: ![Alt text](https://www.example.com/image.jpg)\n",
    "\n",
    "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f3e4ac50-224e-4c94-901d-6ad537cf2a9a"
    }
   },
   "source": [
    "## 2 Loading data\n",
    "\n",
    "[This is a sample dataset from Kaggle, \"Rain in Australia\"](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) \n",
    "\n",
    "Here we will:\n",
    "1. Store the data in a [Spark DataFrame](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "2. From [HDFS](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html) load this file into Spark using csv loading utility \n",
    "    * Open a terminal\n",
    "    * cd to the directory containing the weatherAUS.csv file\n",
    "    * Type: `hdfs dfs -put weatherAUS.csv`\n",
    "    * Check the file was correctly placed by typing: `hdfs dfs -ls -t`\n",
    "3. Analyze the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "347bb50e-3b61-413d-bf6c-db8c582fe89e"
    }
   },
   "outputs": [],
   "source": [
    "print (spark.read                      # The DataFrameReader\n",
    "        .option(\"delimiter\", \"\\t\")     # Use tab delimiter (default is comma-separator)\n",
    "        .option(\"header\", \"true\")      # Use first line of all files as header\n",
    "        .csv('weatherAUS.csv')         # Creates a DataFrame from CSV after reading in the file\n",
    "        .printSchema()\n",
    "      )\n",
    "\n",
    "(spark.read                            # The DataFrameReader\n",
    "        .option(\"delimiter\", \"\\t\")     # Use tab delimiter (default is comma-separator)\n",
    "        .option(\"header\", \"true\")      # Use first line of all files as header\n",
    "        .csv('weatherAUS.csv')         # Creates a DataFrame from CSV after reading in the file        \n",
    ").show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Data Using Spark CSV inference and Correct Delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2bc61cd9-3dd4-48e7-b281-0d69fffb8ea9"
    }
   },
   "outputs": [],
   "source": [
    "(spark.read                            # The DataFrameReader\n",
    "        .option(\"delimiter\", \",\")      # Use tab delimiter (default is comma-separator)\n",
    "        .option(\"header\", \"true\")      # Use first line of all files as header\n",
    "        .option(\"inferSchema\", \"true\") # Use Spark's in built csv inference\n",
    "        .csv('weatherAUS.csv')         # Creates a DataFrame from CSV after reading in the file\n",
    "        .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f0a32f8e-ff1c-453d-983d-2f1163ebfa4b"
    }
   },
   "outputs": [],
   "source": [
    "rainDataset = (spark.read                            # The DataFrameReader\n",
    "                    .option(\"delimiter\", \",\")        # Use tab delimiter (default is comma-separator)\n",
    "                    .option(\"header\", \"true\")        # Use first line of all files as header\n",
    "                    .csv('weatherAUS.csv')           # Creates a DataFrame from CSV after reading in the file\n",
    "              )\n",
    "partitions = rainDataset.rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format( partitions ))\n",
    "print(\"Records: {0:,}\".format( rainDataset.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Convert Dataset to Pandas and view contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 9999\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_row', 10)\n",
    "\n",
    "rainDataset.limit(10).toPandas()\n",
    "#rainDataset.toPandas() --Not Ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0355ef8-5056-4946-b937-cf5214b8b660"
    }
   },
   "source": [
    "### 2.3 Data Cleansing/Correcting\n",
    "\n",
    "[This is a sample dataset from Kaggle, \"Rain in Australia\"](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) \n",
    "\n",
    "Here we will:\n",
    "1. Use [when](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.when) to match 'NA'\n",
    "2. Use [lit](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.lit) function to replace the NA with nulls\n",
    "3. Type Cast the columns to correct data types\n",
    "4. Convert Yes/No to 1/0 \n",
    "\n",
    "Be sure to bookmark Spark SQL API http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f17d697a-f724-44f9-befc-6714a580062d"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col, trim\n",
    "\n",
    "rainDataset_withNulls = rainDataset\n",
    "\n",
    "for c in rainDataset.drop('RainTomorrow', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday').columns:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(c), when(trim(rainDataset[c]) == 'NA', lit(None)).otherwise(rainDataset[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f961edff-4699-4493-8c08-32371e405da8"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rainDataset_withNulls.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2766176c-a0ed-4ad9-945d-a22c6c32a580"
    }
   },
   "outputs": [],
   "source": [
    "#Replace Yes/No with 1/0\n",
    "rainDataset_withNulls = rainDataset_withNulls.withColumn('RainTomorrow', when(rainDataset_withNulls.RainTomorrow == 'Yes', 1).otherwise(0))\n",
    "\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType\n",
    "\n",
    "doubles = ['MinTemp', 'MaxTemp', 'Rainfall','Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RISK_MM']\n",
    "integers = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm']\n",
    "\n",
    "for col in doubles:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(DoubleType()))\n",
    "\n",
    "    \n",
    "for col in integers:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(IntegerType()))\n",
    "\n",
    "rainDataset_withNulls.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aeaf6847-9793-4760-b6ed-2073a82bd5e0"
    }
   },
   "source": [
    "### 2.4 Plotting Data\n",
    "\n",
    "The package we use for plotting here is [matplotlib](https://matplotlib.org/)\n",
    "Here we will:\n",
    "1. Convert Spark DateFrame to Pandas\n",
    "2. Plot the different timeseries on the same plot \n",
    "3. Add BarChart to the same plot, [full tutorial here](http://jonathansoma.com/lede/algorithms-2017/classes/fuzziness-matplotlib/how-pandas-uses-matplotlib-plus-figures-axes-and-subplots/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "51373e56-8c7b-40a7-bed4-2c9872b8f69e"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "#df = rainDataset_withNulls.filter(\"Location IN('Albury', 'MountGinini')\").toPandas()\n",
    "df = rainDataset_withNulls.toPandas()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(20, 10), dpi=300, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "df.groupby('Location').plot(x='Date', y='MaxTemp', ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "38eebabe-ea18-4ae3-919e-e27588a4a346"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(20, 10), dpi=300, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the first section\n",
    "ax1 = fig.add_subplot(211)\n",
    "\n",
    "# Divide the figure into a 2x1 grid, and give me the second section\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "df.groupby('Location').plot(x='Date', y='MaxTemp', ax=ax1, legend=False)\n",
    "df.groupby('Location')['Rainfall'].mean().sort_values().plot(kind='barh', ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preparing Data for PySpark ML\n",
    "\n",
    "In this section we will apply some transformations to the dataset to make it ready for Classification\n",
    "Steps:\n",
    "1. Create a [String Indexer](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer), this will convert String Variables to Integers \n",
    "2. Create a [Pipleline](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.Pipeline) instance and use it to run the String Indexer \n",
    "\n",
    "Bookmark [SparkML API docs!](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#) remember to always use the right version of Spark. Currently we have 2.2 on DDP and 2.3 on AI Lab. Spark Versions can very quite alot from version to version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "00a583dd-cb90-4c16-9347-a7df4c374e5a"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "cols = [item[0] for item in rainDataset_withNulls.drop('Date').dtypes if item[1].startswith('string')]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid='keep')\n",
    "    for c in cols\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "rainDataset_indexed = pipeline.fit(rainDataset_withNulls).transform(rainDataset_withNulls).drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rainDataset_indexed.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#See the target distribution\n",
    "rainDataset_indexed.groupby(\"RainTomorrow\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions  import date_format\n",
    "\n",
    "#See the distribution of data over time\n",
    "rainDataset_indexed.groupby(date_format('Date', 'YYYY').alias('Year')).count().toPandas().plot.bar(x = 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 Preparing Data for PySpark ML\n",
    "\n",
    "In this section we will apply VectorAssembler to the data, this step is a pre-requisite for almost all ML classifiction functions:\n",
    "1. Create a [Vector Assembler](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler), this will convert the columns into 1 Vector\n",
    "2. Create a [Pipleline](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.Pipeline) instance and use it to run the Vector Assembler\n",
    "3. Batch together the transformations we have done on the data so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#This is to create a list of columns we want as features\n",
    "feature_cols = rainDataset_indexed.drop('Date', 'RainTomorrow', 'RISK_MM').columns\n",
    "\n",
    "#This is to initialize the VectorAssembler\n",
    "assembler_features = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "tmp = [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)\n",
    "\n",
    "#Fill Nulls with 0's for now, VectorAssembler doesnt take\n",
    "rainDataset_indexed = rainDataset_indexed.na.fill(0)\n",
    "\n",
    "rainDataset_vectorized = pipeline.fit(rainDataset_indexed).transform(rainDataset_indexed)\n",
    "\n",
    "rainDataset_vectorized.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the stages in the pipeline\n",
    "pipeline_combined = Pipeline(stages=indexers + tmp)\n",
    "\n",
    "#Fill Nulls with 0's for now, VectorAssembler doesnt take\n",
    "rainDataset_combined = rainDataset_withNulls.na.fill(0)\n",
    "\n",
    "rainDataset_vectorized = (pipeline_combined.fit(rainDataset_combined).transform(rainDataset_combined)).drop(*cols)\n",
    "\n",
    "rainDataset_vectorized.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train/Test Splits, Upsampling\n",
    "\n",
    "In this section we will use sampling functions in PySpark to create a balanced Training Set and create a Test Set:\n",
    "1. Use DataFrame [Sample](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html) function, this will allow us to specify how we want to split our dataset\n",
    "2. Use UpSampling with Replacement to balance our Training Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "\n",
    "#Raining Days UpSampling Ratio\n",
    "raining_days_up = 3.4607\n",
    "\n",
    "\n",
    "df_train = unionAll(rainDataset_vectorized.filter(\"RainTomorrow=0\").sample(False, 0.7, seed=randint(100, 999)),\\\n",
    "                    rainDataset_vectorized.filter(\"RainTomorrow=1\").sample(False, 0.7, seed=randint(100, 999)).\\\n",
    "                    sample(True, raining_days_up, seed=randint(100, 999)))\n",
    "\n",
    "\n",
    "df_train.cache()\n",
    "partitions = df_train.rdd.getNumPartitions()\n",
    "print(\"Train Partitions: {0:,}\".format(partitions ))\n",
    "print(\"Train Records: {0:,}\".format(df_train.count()))\n",
    "print(df_train.groupby(\"RainTomorrow\").count().toPandas())\n",
    "\n",
    "df_test = rainDataset_vectorized.join(df_train, df_train.columns, how='left_anti')\n",
    "\n",
    "df_test.cache()\n",
    "partitions = df_test.rdd.getNumPartitions()\n",
    "print(\"Test Partitions: {0:,}\".format( partitions ))\n",
    "print(\"Test Records: {0:,}\".format( df_test.count()))\n",
    "print(df_test.groupby(\"RainTomorrow\").count().toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Modeling\n",
    "\n",
    "In this section we will fit a RanfomForest Model to our Training dataset, and use the Test Data to evaulate it:\n",
    "1. Use [RandomForestModel](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier) to fit a model.\n",
    "2. Score the Test data to get a measure of Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = 10, maxDepth = 5, maxBins = 5)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(df_train.withColumnRenamed(\"RainTomorrow\", \"label\"))\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(df_test)\n",
    "\n",
    "#Print Confusion Matrix\n",
    "predictions.groupby(\"RainTomorrow\", \"prediction\").count().toPandas()\n",
    "#76.58% Model Accuracy, 50,26% True Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 HyperParamter Tuning\n",
    "\n",
    "In this section we will use grid search in PySpark to tune our HyperParameters:\n",
    "1. Create an instance of [BinaryClassificationEvaulator](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator).\n",
    "2. Build a parameter grid using [ParamGridBuilder](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder), add values which we would like to be tested\n",
    "3. Create a [CrossValidator](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) we will use this to evaulate our models, using cross-fold validation\n",
    "\n",
    "Be sure to read through the examples for other models [here!](https://spark.apache.org/docs/2.2.0/ml-tuning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Hyper Parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5, 15])\n",
    "             .addGrid(rf.maxBins, [5, 10])\n",
    "             .addGrid(rf.numTrees, [25, 50])\n",
    "             .build())\n",
    "\n",
    "\n",
    "print(\"Running HP Search\") \n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 2 values for each of maxDepth, maxBins and numTrees\n",
    "# this grid will have 4 x 6 x 3 = 72 parameters ( including settings for CrossValidator)\n",
    "cv = CrossValidator(estimator=rf, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=3)\n",
    "\n",
    "# Run cross validations. This can take some time // To Do, time runs, track them on SparkUI, find ways to make it faster?\n",
    "cvModel = cv.fit(df_train.withColumnRenamed(\"RainTomorrow\", \"label\"))\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(df_test)\n",
    "\n",
    "#Get the best performing Model\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"HP Search Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Diagnostics, Plots\n",
    "\n",
    "In this section we will create a plot for Feature Importances:\n",
    "1. Get the Feature Importances from the Model and plot them\n",
    "2. Create a ROC Curve, get ROC & PR values on Test Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "beta = np.sort(bestModel.featureImportances)\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = bestModel.transform(df_test)\n",
    "\n",
    "#Print Confusion Matrix\n",
    "predictions.groupby(\"RainTomorrow\", \"prediction\").count().toPandas()\n",
    "# 83.16.74% Accurate, 63,34% True Positive Rate, but is ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = predictions.select(['probability', 'RainTomorrow'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is: \", metrics.areaUnderROC)\n",
    "print(\"The PR score is: \", metrics.areaUnderPR)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "%matplotlib inline\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "for i in rainDataset_indexed.drop('Date', 'RainTomorrow', 'RISK_MM').columns:\n",
    "    print str(i) + \" :: \" + str(bestModel.featureImportances[index])\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save the Model and save Reults to HIVE/Parquet\n",
    "\n",
    "In this section we will save the model in HDFS directory. And write the scores to HIVE/Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#View the predictions dataframe\n",
    "predictions.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#Helper Function to get the second element from a Vector\n",
    "secondelement=udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "predictions.select('Date', 'Location_indexed', (secondelement('probability')).alias('probability')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the results in a HIVE table\n",
    "predictions.select('Date', 'Location_indexed', (secondelement('probability')).alias('probability')).createOrReplaceTempView(\"p901shm_ddp_intro_rain_austrialla_results\")\n",
    "\n",
    "HIVE_SQL = \"DROP TABLE IF EXISTS ddp_cvm.p901shm_ddp_intro_rain_austrialla_results\"\n",
    "spark.sql(HIVE_SQL)\n",
    "\n",
    "HIVE_SQL = \"CREATE TABLE ddp_cvm.p901shm_ddp_intro_rain_austrialla_results AS SELECT * FROM p901shm_ddp_intro_rain_austrialla_results\"\n",
    "spark.sql(HIVE_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the results as a Parquet File\n",
    "predictions.select('Date', 'Location_indexed', (secondelement('probability')).alias('probability')).write.parquet(\"p901shm_ddp_intro_rain_austrialla_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some HIVE related Tips\n",
    "\n",
    "#Read A table as a dataframe:\n",
    "#spark.table(\"ddp_cvm.p901shm_ddp_intro_rain_austrialla_results\").printSchema()\n",
    "df_ddp = spark.table(\"ddp_cvm.p901shm_ddp_intro_rain_austrialla_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Next Steps!\n",
    "\n",
    "1. Use [OneHotEncoding](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder) instead of StringIndexer\n",
    "2. Use [Imputer](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer) instead of just filling in missing values with 0's. Improving the quality of your training data set is key. You can try inputations for each location seperatly\n",
    "3. Try [TimeSeries](https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b) follow this Tutorial\n",
    "4. Follow [this](https://towardsdatascience.com/tuning-hyperparameters-part-i-successivehalving-c6c602865619) tutorial on how to HyperParameter Tune"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Use This pySpark (Spark 2.2)",
   "language": "python",
   "name": "usethis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nbpresent": {
   "slides": {
    "b5d2efcd-aa4b-44d8-8da3-3c291c46ac2d": {
     "id": "b5d2efcd-aa4b-44d8-8da3-3c291c46ac2d",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
