{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body style=\"background-color:powderblue;\">\n",
    "<p><i>Author: Shaheer Mansoor</i></p>\n",
    "\n",
    "## Code Packaging\n",
    "\n",
    "**This code provides some basic functionality, you can use this to**\n",
    "\n",
    "1. Read what enviorment the code is running on\n",
    "2. Read parameters from a json file given your enviorment\n",
    "3. Log, this is basic logging utility including log file creation. You can also look into python's built in logging as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "    Starting Model Execution\n",
      "----------------------------------\n",
      "Trying to import all the necessary libraries required to start a log file\n",
      "RunDate Set to :: 2019-04-18\n",
      "[2019-04-18 11:16:56.635314] Log file is created here /data/home/p901shm/projects/Spark-DDP Introduction/ModelRun_2019-04-18.log\n",
      "[2019-04-18 11:16:56.635520] Current Working Directory is set to: /data/home/p901shm/projects/Spark-DDP Introduction\n",
      "[2019-04-18 11:16:56.635747] Current host-name is : sb-hdp-e4 and current enviorment is set to: ddp\n",
      "[2019-04-18 11:16:56.636011] All Good! Starting execution\n",
      "[2019-04-18 11:16:56.636189] Getting RainData Set\n",
      "[2019-04-18 11:17:01.973337] Score Records: 142,193\n",
      "[2019-04-18 11:17:02.799122] Data Cleansing Step 1: Convert Yes/No to 1/0\n",
      "[2019-04-18 11:17:02.860775] Data Cleansing Step 2: Cast Varibles to correct datatypes\n",
      "[2019-04-18 11:17:02.963356] Casted ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RISK_MM'] to doubles\n",
      "[2019-04-18 11:17:03.036044] Casted ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm'] to integers\n",
      "[2019-04-18 11:17:03.036520] Prep Data for Model Step 3: Applying StringIndexers\n",
      "[2019-04-18 11:17:28.885501] Prep Data for Model Step 3: Applying VectorAssembler\n",
      "[2019-04-18 11:17:29.481235] Score Partitions: 4\n",
      "[2019-04-18 11:17:35.234878] Score Records: 142,193\n",
      "[2019-04-18 11:17:35.235170] Scoring Step 4: Loading RandomForest Model\n",
      "[2019-04-18 11:17:44.702139] Scoring Samples\n",
      "[2019-04-18 11:17:46.098551] Scored Partitions: 4\n",
      "[2019-04-18 11:17:59.126480] Scored Records: 142,193\n",
      "[2019-04-18 11:17:59.127097] Save results Step 5: Writing Scores to HIVE\n",
      "[2019-04-18 11:18:12.545194] All Done! Exiting\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Define Global Variables here\n",
    "import_flag = 0\n",
    "env = 0\n",
    "cwd = 0\n",
    "log_file_name = 0\n",
    "log_hive_table = 0\n",
    "Live_Run = False\n",
    "\n",
    "import traceback \n",
    "\n",
    "try:\n",
    "\n",
    "    #Python\n",
    "    import os\n",
    "    import sys    \n",
    "    import json\n",
    "    import socket    \n",
    "    import traceback   \n",
    "    import pandas as pd\n",
    "    from subprocess import call\n",
    "    from datetime import datetime, date, time, timedelta         \n",
    "    \n",
    "    #Spark ML\n",
    "    from pyspark.ml import Pipeline, PipelineModel          \n",
    "    from pyspark.ml.classification import RandomForestClassificationModel\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "    from pyspark.ml.feature import VectorIndexer, StringIndexer, VectorAssembler, OneHotEncoder, Imputer, StandardScaler\n",
    "    \n",
    "    #Spark MLLib\n",
    "    from pyspark.mllib.stat import Statistics  \n",
    "    \n",
    "    #Spark Context\n",
    "    from pyspark.context import SparkContext\n",
    "    \n",
    "    #Spark SQL\n",
    "    from pyspark.sql.session import SparkSession      \n",
    "    from pyspark.sql import Window, DataFrame\n",
    "    from pyspark.sql.functions import *   \n",
    "    from pyspark.sql.functions import lit, col, broadcast, udf             \n",
    "    from pyspark.sql.types import DoubleType, FloatType, IntegerType, StringType\n",
    "    \n",
    "except ImportError as E:\n",
    "    print(\"Imports Failed : \" + str(E))\n",
    "    global import_flag\n",
    "    import_flag = -1\n",
    "\n",
    "    \n",
    "#Spark/HIVE Variables\n",
    "'''\n",
    "global spark\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"The RainMaker\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "# set the log level to one of ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN (default INFO)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark-submit --name RainMaker --executor-memory 5G --master yarn --driver-memory 5G --executor-cores 10 --num-executors 15 --conf spark.dynamicAllocation.minExecutors=10 --conf spark.dynamicAllocation.initialExecutors=10 rainmaker.py \n",
    "'''\n",
    "\n",
    "\n",
    "#Global Optimization Parameters\n",
    "#Read about other parameters here(https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior)    \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")  \n",
    "spark.conf.set(\"spark.default.parallelism\", \"10\")\n",
    "spark.conf.set(\"spark.app.name\", \"The RainMaker\")  \n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                              This is the logging fucntion\n",
    "#------------------------------------------------------------------------------------------------------------------------------------   \n",
    "def Logger(string):\n",
    "    #from datetime import datetime\n",
    "    try:\n",
    "        if val is None: # The variable\n",
    "            pass            \n",
    "    except: # Here we write to the log file\n",
    "        print(\"[\" + str(datetime.now()) + \"] \" + str(string))\n",
    "        with open(log_file_name, \"a\") as myfile:\n",
    "            myfile.write(\"[\" + str(datetime.now()) + \"] \" + str(string) + \"\\n\")\n",
    "    \n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           This sets up the log file   \n",
    "#------------------------------------------------------------------------------------------------------------------------------------   \n",
    "def Set_LogFile():\n",
    "    #from datetime import date\n",
    "    try:\n",
    "        \n",
    "        global log_file_name\n",
    "        log_file_name = str(cwd) + \"/ModelRun_\" + str(date.today()) + \".log\"\n",
    "        return 0\n",
    "    except:\n",
    "        print(\"Errored out during setting up log file\")\n",
    "        traceback.print_exc()\n",
    "        return -1\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           This function Tests the JSON File Exists\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "def Read_JSON_File():       \n",
    "    \n",
    "    r_value = 0    \n",
    "    try:\n",
    "        with open(str(cwd) + \"/data.json\", 'r') as stream:\n",
    "            y_list = json.load(stream)\n",
    "            stream.close()\n",
    "            \n",
    "    except IOError as exc:\n",
    "        print(exc)\n",
    "        traceback.print_exc()\n",
    "        Logger(str(exc))\n",
    "        r_value = -1\n",
    "    \n",
    "    return r_value\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           This function can read the JSON file and returns requested values versus a key\n",
    "#                           This Function also handles the dates for Live and Daignostic Runs\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "def Get_From_JSON(env, key):       \n",
    "    global Live_Run\n",
    "    r_value = 0\n",
    "    \n",
    "    \n",
    "    try:        \n",
    "        with open(str(cwd) + \"/data.json\", 'r') as stream:\n",
    "            \n",
    "            j_list = json.load(stream)                             \n",
    "            return j_list[env][key]    \n",
    "            \n",
    "    except ValueError as exc:\n",
    "        print(exc)      \n",
    "        traceback.print_exc()\n",
    "        Logger(str(exc))\n",
    "        r_value = -1\n",
    "    except:\n",
    "        print(\"Unexpected error on key:\" + str(key), sys.exc_info()[0])\n",
    "\n",
    "        \n",
    "    stream.close()\n",
    "    return r_value\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           Set Enviorment\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "def Set_Env():\n",
    "    try:\n",
    "        global env\n",
    "        if \"sb-hdp-e\" in str(socket.gethostname()):\n",
    "            env = \"ddp\"\n",
    "        if \"sb-hdpdev\" in str(socket.gethostname()):\n",
    "            env = \"hdp_dev\"            \n",
    "        if \"sb-hdppra\" in str(socket.gethostname()):\n",
    "            env = \"hdp_prd\"\n",
    "        if \"sh-hdpts\" in str(socket.gethostname()):\n",
    "            env = \"hdp_test\"\n",
    "        \n",
    "        return 0\n",
    "    except Exception as e:    \n",
    "        Logger(\"Exception Trace: \", e)\n",
    "        traceback.print_exc()\n",
    "        Logger(\"Errored out during setting up Enviorment\")\n",
    "        Logger(traceback.format_exc())\n",
    "        return -1\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           Get Enviorment\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "def Get_Env():\n",
    "    try:\n",
    "        global env\n",
    "        return env\n",
    "    except Exception as e:    \n",
    "        Logger(\"Exception Trace: \", e)\n",
    "        traceback.print_exc()\n",
    "        Logger(\"Errored out getting Enviorment\")\n",
    "        Logger(traceback.format_exc())\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           Helper UDF\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "secondelement=udf(lambda v:float(v[1]),FloatType())\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                           Create Feature Weights with Names\n",
    "#------------------------------------------------------------------------------------------------------------------------------------  \n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                     Main Entry Point\n",
    "#------------------------------------------------------------------------------------------------------------------------------------       \n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        \n",
    "        print(\"----------------------------------\")\n",
    "        print(\"    Starting Model Execution\")\n",
    "        print(\"----------------------------------\")\n",
    "        print(\"Trying to import all the necessary libraries required to start a log file\")\n",
    "        \n",
    "        #Set Working Directory\n",
    "        global cwd\n",
    "        cwd = os.getcwd()        \n",
    "        \n",
    "        #Set RunDate                     \n",
    "        Run_Date = str(datetime.strftime(datetime.today(), '%Y-%m-%d')) + ''\n",
    "        print(\"RunDate Set to :: \" + str(Run_Date))\n",
    "\n",
    "        #Set Log File \n",
    "        has_logfile = Set_LogFile()        \n",
    "        Logger(\"Log file is created here \" + str(log_file_name)) \n",
    "        Logger(\"Current Working Directory is set to: \" + str(cwd)) \n",
    "\n",
    "        #Set Environment Name\n",
    "        has_env = Set_Env()        \n",
    "        Logger(\"Current host-name is : \" + str(socket.gethostname()) + \" and current enviorment is set to: \" + str(Get_Env()))         \n",
    "\n",
    "        #Read JSON File, if this fails the Model will exit\n",
    "        has_json = Read_JSON_File() \n",
    "        \n",
    "        if has_logfile == 0 and has_json == 0 and has_env == 0:\n",
    "            Logger(\"All Good! Starting execution\")\n",
    "            \n",
    "            #Get Data\n",
    "            Logger(\"Getting RainData Set\")\n",
    "            rainDataset_withNulls = spark.table(Get_From_JSON(str(Get_Env()), 'read_db') + '.' + Get_From_JSON(str(Get_Env()), 'Input_Table'))\n",
    "            Logger(\"Score Records: {0:,}\".format(rainDataset_withNulls.count()))\n",
    "\n",
    "            for c in rainDataset_withNulls.drop('RainTomorrow', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday').columns:\n",
    "                rainDataset_withNulls = rainDataset_withNulls.withColumn(str(c), when(trim(rainDataset_withNulls[c]) == 'NA', lit(None)).otherwise(rainDataset_withNulls[c]))\n",
    "\n",
    "            #Get Data\n",
    "            Logger(\"Data Cleansing Step 1: Convert Yes/No to 1/0\")\n",
    "            #Replace Yes/No with 1/0\n",
    "            rainDataset_withNulls = rainDataset_withNulls.withColumn('RainTomorrow', when(rainDataset_withNulls.RainTomorrow == 'Yes', 1).otherwise(0))\n",
    "\n",
    "            Logger(\"Data Cleansing Step 2: Cast Varibles to correct datatypes\")\n",
    "            doubles = ['MinTemp', 'MaxTemp', 'Rainfall','Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RISK_MM']\n",
    "            integers = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm']\n",
    "\n",
    "            for col in doubles:\n",
    "                rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(DoubleType()))\n",
    "            Logger(\"Casted \" + str(doubles) + \" to doubles\")\n",
    "\n",
    "            for col in integers:\n",
    "                rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(IntegerType()))\n",
    "            Logger(\"Casted \" + str(integers) + \" to integers\")\n",
    "\n",
    "            Logger(\"Prep Data for Model Step 3: Applying StringIndexers\")\n",
    "            cols = [item[0] for item in rainDataset_withNulls.drop('Date').dtypes if item[1].startswith('string')]\n",
    "\n",
    "            indexers = [\n",
    "                StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid='keep')\n",
    "                for c in cols\n",
    "            ]\n",
    "\n",
    "            pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "            rainDataset_indexed = pipeline.fit(rainDataset_withNulls).transform(rainDataset_withNulls).drop(*cols)\n",
    "\n",
    "            Logger(\"Prep Data for Model Step 3: Applying VectorAssembler\")\n",
    "            #This is to create a list of columns we want as features\n",
    "            feature_cols = rainDataset_indexed.drop('Date', 'RainTomorrow', 'RISK_MM').columns\n",
    "\n",
    "            #This is to initialize the VectorAssembler\n",
    "            assembler_features = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "            tmp = [assembler_features]\n",
    "            pipeline = Pipeline(stages=tmp)\n",
    "\n",
    "            #Fill Nulls with 0's for now, VectorAssembler doesnt take\n",
    "            rainDataset_indexed = rainDataset_indexed.na.fill(0)\n",
    "\n",
    "            rainDataset_score = pipeline.fit(rainDataset_indexed).transform(rainDataset_indexed)\n",
    "\n",
    "            rainDataset_score.cache()\n",
    "            partitions = rainDataset_score.rdd.getNumPartitions()\n",
    "            Logger(\"Score Partitions: {0:,}\".format(partitions ))\n",
    "            Logger(\"Score Records: {0:,}\".format(rainDataset_score.count()))\n",
    "\n",
    "            Logger(\"Scoring Step 4: Loading RandomForest Model\")  \n",
    "            rf = RandomForestClassificationModel.load(Get_From_JSON(str(Get_Env()), 'Model_File'))\n",
    "\n",
    "            Logger(\"Scoring Samples\")  \n",
    "            predictions = rf.transform(rainDataset_score)\n",
    "            predictions.cache()\n",
    "            partitions = rainDataset_score.rdd.getNumPartitions()\n",
    "            Logger(\"Scored Partitions: {0:,}\".format(partitions ))\n",
    "            Logger(\"Scored Records: {0:,}\".format(predictions.count()))\n",
    "\n",
    "            Logger(\"Save results Step 5: Writing Scores to HIVE\")  \n",
    "            predictions.select('Date', 'Location_indexed', (secondelement('probability')).alias('probability')).createOrReplaceTempView(\"p901shm_rain_austrialla_scores\")\n",
    "            spark.sql(\"DROP TABLE IF EXISTS \" + str(Get_From_JSON(str(Get_Env()), 'read_db')) + \".\" + str(Get_From_JSON(str(Get_Env()), 'Output_Table')))\n",
    "            spark.sql(\"CREATE TABLE \" + str(Get_From_JSON(str(Get_Env()), 'read_db')) + \".\" + str(Get_From_JSON(str(Get_Env()), 'Output_Table')) + \" AS SELECT * FROM p901shm_rain_austrialla_scores\")\n",
    "            \n",
    "            Logger(\"All Done! Exiting\") \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Errored Out in main\")\n",
    "        print(\"Exception Trace: \", e)\n",
    "        traceback.print_exc()\n",
    "        Logger(traceback.format_exc())\n",
    "        return -1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hostname to define different enviorments\n",
    "\n",
    "1. We can read host name patterns. Use these to define enviorment specific parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddp\n"
     ]
    }
   ],
   "source": [
    "print str(Get_Env())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Json file to get enviorment variables\n",
    "\n",
    "1. We can use our enviorment variables to pick values from our Json files\n",
    "2. Ideally we should never hardcode these parameters (especially the ones which change across platforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddp_cvm\n",
      "p901shm_rain_austrialla_dataset\n"
     ]
    }
   ],
   "source": [
    "print Get_From_JSON(str(Get_Env()), 'read_db')\n",
    "print Get_From_JSON(str(Get_Env()), 'Input_Table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try - Catch\n",
    "\n",
    "1. We can catch our expections to narrow down what went wrong. This is not important within a notebook enviorment but when we run the models via scripts on other enviorments this can help in debugging\n",
    "2. We write code in a try block, if something goes wrong the code jumps to catch block. If we use functions to seperate our code blocks we can write exceptions which can be meaningful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-18 11:13:14.121229] Errored Out on getting table\n",
      "[2019-04-18 11:13:14.123385] Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-d66f89e83a55>\", line 4, in <module>\n",
      "    df = spark.table(\"ddp_cvm.p901shm_rain_austrialla_scoresd\")\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 616, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self._wrapped)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u\"Table or view not found: `ddp_cvm`.`p901shm_rain_austrialla_scoresd`;;\\n'UnresolvedRelation `ddp_cvm`.`p901shm_rain_austrialla_scoresd`\\n\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-d66f89e83a55>\", line 4, in <module>\n",
      "    df = spark.table(\"ddp_cvm.p901shm_rain_austrialla_scoresd\")\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 616, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self._wrapped)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u\"Table or view not found: `ddp_cvm`.`p901shm_rain_austrialla_scoresd`;;\\n'UnresolvedRelation `ddp_cvm`.`p901shm_rain_austrialla_scoresd`\\n\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "try:\n",
    "    df = spark.table(\"ddp_cvm.p901shm_rain_austrialla_scoresd\")    \n",
    "    Logger(\"All Went Ok!\")\n",
    "    \n",
    "except Exception as e:      \n",
    "    Logger(\"Errored Out on getting table\")\n",
    "    traceback.print_exc()\n",
    "    Logger(traceback.format_exc())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Table or view not found: `ddp_cvm`.`p901shm_rain_austrialla_scoresss`;;\\n'UnresolvedRelation `ddp_cvm`.`p901shm_rain_austrialla_scoresss`\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d7f23ff2b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ddp_cvm.p901shm_rain_austrialla_scoresss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \"\"\"\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Table or view not found: `ddp_cvm`.`p901shm_rain_austrialla_scoresss`;;\\n'UnresolvedRelation `ddp_cvm`.`p901shm_rain_austrialla_scoresss`\\n\""
     ]
    }
   ],
   "source": [
    "spark.table(\"ddp_cvm.p901shm_rain_austrialla_scoresss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Optimize Code\n",
    "\n",
    "1. You can use the notebook to optimize individual parts of your code\n",
    "2. Once this is done we can run it within the main, and then create a py file for Spark-Submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-17 14:40:21.917710] Getting RainData Set\n",
      "[2019-04-17 14:40:22.349886] Score Records: 142,193\n",
      "[2019-04-17 14:40:22.575853] Data Cleansing Step 1: Convert Yes/No to 1/0\n",
      "[2019-04-17 14:40:22.588577] Data Cleansing Step 2: Cast Varibles to correct datatypes\n",
      "[2019-04-17 14:40:22.657166] Casted ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RISK_MM'] to doubles\n",
      "[2019-04-17 14:40:22.734683] Casted ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm'] to integers\n",
      "[2019-04-17 14:40:22.735127] Prep Data for Model Step 3: Applying StringIndexers\n",
      "[2019-04-17 14:40:23.703604] Prep Data for Model Step 3: Applying VectorAssembler\n",
      "[2019-04-17 14:40:23.892766] Score Partitions: 4\n",
      "[2019-04-17 14:40:27.360463] Score Records: 142,193\n",
      "[2019-04-17 14:40:27.361020] Scoring Step 4: Loading RandomForest Model\n",
      "[2019-04-17 14:40:36.609197] Scoring Samples\n",
      "[2019-04-17 14:40:38.037217] Scored Partitions: 4\n",
      "[2019-04-17 14:40:50.177543] Scored Records: 142,193\n",
      "[2019-04-17 14:40:50.178060] Save results Step 5: Writing Scores to HIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Data\n",
    "Logger(\"Getting RainData Set\")\n",
    "rainDataset_withNulls = spark.table(Get_From_JSON(str(Get_Env()), 'read_db') + '.' + Get_From_JSON(str(Get_Env()), 'Input_Table'))\n",
    "Logger(\"Score Records: {0:,}\".format(rainDataset_withNulls.count()))\n",
    "\n",
    "for c in rainDataset_withNulls.drop('RainTomorrow', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday').columns:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(c), when(trim(rainDataset_withNulls[c]) == 'NA', lit(None)).otherwise(rainDataset_withNulls[c]))\n",
    "\n",
    "#Get Data\n",
    "Logger(\"Data Cleansing Step 1: Convert Yes/No to 1/0\")\n",
    "#Replace Yes/No with 1/0\n",
    "rainDataset_withNulls = rainDataset_withNulls.withColumn('RainTomorrow', when(rainDataset_withNulls.RainTomorrow == 'Yes', 1).otherwise(0))\n",
    "\n",
    "Logger(\"Data Cleansing Step 2: Cast Varibles to correct datatypes\")\n",
    "doubles = ['MinTemp', 'MaxTemp', 'Rainfall','Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RISK_MM']\n",
    "integers = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm']\n",
    "\n",
    "for col in doubles:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(DoubleType()))\n",
    "Logger(\"Casted \" + str(doubles) + \" to doubles\")\n",
    "\n",
    "for col in integers:\n",
    "    rainDataset_withNulls = rainDataset_withNulls.withColumn(str(col), rainDataset_withNulls[col].cast(IntegerType()))\n",
    "Logger(\"Casted \" + str(integers) + \" to integers\")\n",
    "\n",
    "Logger(\"Prep Data for Model Step 3: Applying StringIndexers\")\n",
    "cols = [item[0] for item in rainDataset_withNulls.drop('Date').dtypes if item[1].startswith('string')]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid='keep')\n",
    "    for c in cols\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "rainDataset_indexed = pipeline.fit(rainDataset_withNulls).transform(rainDataset_withNulls).drop(*cols)\n",
    "\n",
    "Logger(\"Prep Data for Model Step 3: Applying VectorAssembler\")\n",
    "#This is to create a list of columns we want as features\n",
    "feature_cols = rainDataset_indexed.drop('Date', 'RainTomorrow', 'RISK_MM').columns\n",
    "\n",
    "#This is to initialize the VectorAssembler\n",
    "assembler_features = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "tmp = [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)\n",
    "\n",
    "#Fill Nulls with 0's for now, VectorAssembler doesnt take\n",
    "rainDataset_indexed = rainDataset_indexed.na.fill(0)\n",
    "\n",
    "rainDataset_score = pipeline.fit(rainDataset_indexed).transform(rainDataset_indexed)\n",
    "\n",
    "rainDataset_score.cache()\n",
    "partitions = rainDataset_score.rdd.getNumPartitions()\n",
    "Logger(\"Score Partitions: {0:,}\".format(partitions ))\n",
    "Logger(\"Score Records: {0:,}\".format(rainDataset_score.count()))\n",
    "\n",
    "Logger(\"Scoring Step 4: Loading RandomForest Model\")  \n",
    "rf = RandomForestClassificationModel.load(Get_From_JSON(str(Get_Env()), 'Model_File'))\n",
    "\n",
    "Logger(\"Scoring Samples\")  \n",
    "predictions = rf.transform(rainDataset_score)\n",
    "predictions.cache()\n",
    "partitions = rainDataset_score.rdd.getNumPartitions()\n",
    "Logger(\"Scored Partitions: {0:,}\".format(partitions ))\n",
    "Logger(\"Scored Records: {0:,}\".format(predictions.count()))\n",
    "\n",
    "Logger(\"Save results Step 5: Writing Scores to HIVE\")  \n",
    "predictions.select('Date', 'Location_indexed', (secondelement('probability')).alias('probability')).createOrReplaceTempView(\"p901shm_rain_austrialla_scores\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + str(Get_From_JSON(str(Get_Env()), 'read_db')) + \".\" + str(Get_From_JSON(str(Get_Env()), 'Output_Table')))\n",
    "spark.sql(\"CREATE TABLE \" + str(Get_From_JSON(str(Get_Env()), 'read_db')) + \".\" + str(Get_From_JSON(str(Get_Env()), 'Output_Table')) + \" AS SELECT * FROM p901shm_rain_austrialla_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps!\n",
    "\n",
    "1. Read more about on [spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)\n",
    "2. Read about [parameters](https://spark.apache.org/docs/latest/configuration.html) you can configure "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Use This pySpark (Spark 2.2)",
   "language": "python",
   "name": "usethis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
