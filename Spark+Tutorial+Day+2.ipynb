{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body style=\"background-color:powderblue;\">\n",
    "<p><i>Author: Shaheer Mansoor</i></p>\n",
    "\n",
    "\n",
    "## 1. Spark Transformations & Actions\n",
    "\n",
    "In this section we will run some transformations and actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_1554311896068_9231\n"
     ]
    }
   ],
   "source": [
    "print sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"ddp_cvm.uc5_ads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken 00:00:01.43\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#spark.table(\"ddp_cvm.uc5_ads\").printSchema()\n",
    "df = spark.table(\"ddp_cvm.uc5_ads\")\n",
    "df = df.select(\"mortgage_lapsed_mortages\")\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2088799\n",
      "Time Taken 00:00:18.78\n"
     ]
    }
   ],
   "source": [
    "#df.explain()\n",
    "start = time.time()\n",
    "\n",
    "print df.count()\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Joins\n",
    "\n",
    "In this section we join two tables in Spark\n",
    "\n",
    "We will take the customer table and transaction for 201801 onwards and calculate the Sum for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12551554\n",
      "Time Taken 00:00:12.85\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#spark.table(\"ddp_cvm.uc5_pop_distinct\").printSchema()\n",
    "df_cust = spark.table(\"ddp_central.src_sdw_part_party\").filter(\"validtodate = '2999-12-31'\").\\\n",
    "                select('partygenid').distinct()\n",
    "print df_cust.count()\n",
    "#12,551,554 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223568890\n",
      "Time Taken 00:00:11.63\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_trx = spark.table(\"ddp_central.depo_transaction_t05fa\")\\\n",
    "        .select('primaryaccountholder', 'transactionmonth', 'amount')\\\n",
    "        .filter(\"transactionmonth = 201801\")\n",
    "print df_trx.count()\n",
    "#223,568,890 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explain the query plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[partygenid#3062], functions=[sum(amount#3174)])\n",
      "+- *HashAggregate(keys=[partygenid#3062], functions=[partial_sum(amount#3174)])\n",
      "   +- *Project [amount#3174, partygenid#3062]\n",
      "      +- *SortMergeJoin [primaryaccountholder#3169], [partygenid#3062], Inner\n",
      "         :- *Sort [primaryaccountholder#3169 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(primaryaccountholder#3169, 200)\n",
      "         :     +- *Filter isnotnull(primaryaccountholder#3169)\n",
      "         :        +- HiveTableScan [primaryaccountholder#3169, amount#3174], HiveTableRelation `ddp_central`.`depo_transaction_t05fa`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [agreementgenid#3163, transactiondate#3164, transactiongenid#3165, transtypecd#3166, servicetime#3167, bankid#3168, primaryaccountholder#3169, productgenid#3170, performedbyorggenid#3171, channeltypecd#3172, channelgenid#3173, amount#3174, originalamount#3175, originalcurrencycd#3176, accountingtypecd#3177, secpartygenid#3178, secpartyrolecd#3179, secaccounttypecd#3180, secagreementgenid#3181, secproductgenid#3182, secclearingnum#3183, secaccountnum#3184, seccountrycd#3185, relatedhendelsegenid#3186, ... 11 more fields], [transactionmonth#3198], [isnotnull(transactionmonth#3198), (transactionmonth#3198 = 201801)]\n",
      "         +- *Sort [partygenid#3062 ASC NULLS FIRST], false, 0\n",
      "            +- *HashAggregate(keys=[partygenid#3062], functions=[])\n",
      "               +- Exchange hashpartitioning(partygenid#3062, 200)\n",
      "                  +- *HashAggregate(keys=[partygenid#3062], functions=[])\n",
      "                     +- *Project [partygenid#3062]\n",
      "                        +- *Filter ((isnotnull(validtodate#3101) && (validtodate#3101 = 2999-12-31)) && isnotnull(partygenid#3062))\n",
      "                           +- HiveTableScan [partygenid#3062, validtodate#3101], HiveTableRelation `ddp_central`.`src_sdw_part_party`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [partygenid#3062, customernumtypecd#3063, pep#3064, birthdate#3065, age#3066, gender#3067, parishcd#3068, deceased#3069, deceasedregdate#3070, deceaseddate#3071, bankruptcydate#3072, bankruptcyenddate#3073, debtrestructuringdate#3074, debtrestructuringenddate#3075, inabsenteeregister#3076, advertisingfilter#3077, bankid#3078, hascontactperson#3079, customerbasemanager#3080, homebranchgenid#3081, customerstartdate#3082, typeofperson#3083, customerstatus#3084, newcustomer#3085, ... 20 more fields]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_joined = df_trx.join(df_cust, df_trx.primaryaccountholder == df_cust.partygenid, how = 'inner')\n",
    "\n",
    "df_sums = df_joined.groupby('partygenid').agg(sum(\"amount\"))\n",
    "df_sums.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run the join \n",
    "(Dont Run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223568890\n",
      "Time Taken 00:00:31.00\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_joined = df_trx.join(df_cust, df_trx.primaryaccountholder == df_cust.partygenid, 'inner')\n",
    "\n",
    "print df_joined.count()\n",
    "#223,568,890 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4633019\n",
      "+--------------------+------------+\n",
      "|          partygenid| sum(amount)|\n",
      "+--------------------+------------+\n",
      "|[00 3E 86 81 9C 6...|   892.50000|\n",
      "|[00 4D 5A 62 70 A...| 12965.62000|\n",
      "|[00 8D 6C 37 46 7...| 11908.67000|\n",
      "|[00 A9 17 96 EB C...| 12300.18000|\n",
      "|[00 B5 89 00 1E A...| -1997.81000|\n",
      "|[00 BC 11 29 3A 1...|  1673.05000|\n",
      "|[00 EB 4C 4A 0B 0...|-59059.97000|\n",
      "|[01 92 84 C7 25 1...|-64634.91000|\n",
      "|[01 B0 F1 4B 44 A...|   420.57000|\n",
      "|[01 E6 CF D5 39 7...|-10065.58000|\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "Time Taken 00:00:31.06\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast, sum\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_sums = df_joined.groupby('partygenid').agg(sum(\"amount\"))\n",
    "print df_sums.count()\n",
    "print df_sums.show(10)\n",
    "#4,633,019 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run the join again, this time we will cache our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223568890\n",
      "Time Taken 00:00:20.27\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_joined = df_trx.join(df_cust, df_trx.primaryaccountholder == df_cust.partygenid, 'inner').cache()\n",
    "print df_joined.count()\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4633019\n",
      "+--------------------+------------+\n",
      "|          partygenid| sum(amount)|\n",
      "+--------------------+------------+\n",
      "|[00 3E 86 81 9C 6...|   892.50000|\n",
      "|[00 4D 5A 62 70 A...| 12965.62000|\n",
      "|[00 8D 6C 37 46 7...| 11908.67000|\n",
      "|[00 A9 17 96 EB C...| 12300.18000|\n",
      "|[00 B5 89 00 1E A...| -1997.81000|\n",
      "|[00 BC 11 29 3A 1...|  1673.05000|\n",
      "|[00 EB 4C 4A 0B 0...|-59059.97000|\n",
      "|[01 92 84 C7 25 1...|-64634.91000|\n",
      "|[01 B0 F1 4B 44 A...|   420.57000|\n",
      "|[01 E6 CF D5 39 7...|-10065.58000|\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "Time Taken 00:00:01.41\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast, sum\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_sums = df_joined.groupby('partygenid').agg(sum(\"amount\"))\n",
    "print df_sums.count()\n",
    "print df_sums.show(10)\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Cache the tables after reading them into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12551554\n",
      "Time Taken 00:00:06.28\n"
     ]
    }
   ],
   "source": [
    "df_joined.unpersist(True)\n",
    "\n",
    "import pyspark\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#spark.table(\"ddp_cvm.uc5_pop_distinct\").printSchema()\n",
    "df_cust_cached = spark.table(\"ddp_central.src_sdw_part_party\").filter(\"validtodate = '2999-12-31'\").\\\n",
    "                       select('partygenid').distinct().cache()\n",
    "print df_cust_cached.count()\n",
    "#12,551,554 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n",
      "223568890\n",
      "Time Taken 00:00:06.70\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#spark.table(\"ddp_cvm.uc5_pop_distinct\").printSchema()\n",
    "df_trx_cached = spark.table(\"ddp_central.depo_transaction_t05fa\")\\\n",
    "        .selectExpr('primaryaccountholder as partygenid', 'amount')\\\n",
    "        .filter(\"transactionmonth = 201801\")\n",
    "        \n",
    "df_trx_cached.cache()\n",
    "#df_trx_cached.rdd.persist(pyspark.StorageLevel.MEMORY_ONLY_2)\n",
    "print df_trx_cached.rdd.getStorageLevel()\n",
    "#Action\n",
    "print df_trx_cached.count()\n",
    "#223,568,890 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4633019\n",
      "+--------------------+------------+\n",
      "|          partygenid| sum(amount)|\n",
      "+--------------------+------------+\n",
      "|[00 3E 86 81 9C 6...|   892.50000|\n",
      "|[00 4D 5A 62 70 A...| 12965.62000|\n",
      "|[00 8D 6C 37 46 7...| 11908.67000|\n",
      "|[00 A9 17 96 EB C...| 12300.18000|\n",
      "|[00 B5 89 00 1E A...| -1997.81000|\n",
      "|[00 BC 11 29 3A 1...|  1673.05000|\n",
      "|[00 EB 4C 4A 0B 0...|-59059.97000|\n",
      "|[01 92 84 C7 25 1...|-64634.91000|\n",
      "|[01 B0 F1 4B 44 A...|   420.57000|\n",
      "|[01 E6 CF D5 39 7...|-10065.58000|\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "Time Taken 00:00:14.80\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast, sum\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_joined_c = df_trx_cached.join(df_cust_cached, ['partygenid'], 'inner')\n",
    "df_sums = df_joined_c.groupby('partygenid').agg(sum(\"amount\"))\n",
    "#Action\n",
    "print df_sums.count()\n",
    "print df_sums.show(10)\n",
    "#4,633,019 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#partitions = df_trx_cached.rdd.getNumPartitions()\n",
    "#print(\"Partitions: {0:,}\".format(partitions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the join again, after repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5032428\n",
      "Time Taken 00:01:10.59\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import broadcast\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_joined_c = df_trx_cached.join(df_cust_cached, ['partygenid'], 'inner')\n",
    "df_sums = df_joined_c.groupby('partygenid').agg(sum(\"amount\"))\n",
    "\n",
    "#Action\n",
    "print df_sums.count()\n",
    "#4,633,019 rows\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time Taken {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adjust Partitioning to improve Join Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 45\n",
      "Records: 1,418,059,301\n",
      "Partitions: 45\n",
      "Records: 12,551,554\n"
     ]
    }
   ],
   "source": [
    "#100, 500, 1000, 1500, 2000\n",
    "\n",
    "df_trx_cached = spark.table(\"ddp_central.depo_transaction_t05fa\")\\\n",
    "        .selectExpr('primaryaccountholder as partygenid', 'amount')\\\n",
    "        .filter(\"transactionmonth > 201809\")       \n",
    "df_trx_cached.cache()\n",
    "\n",
    "df_trx_cached = df_trx_cached.repartition(45)\n",
    "\n",
    "partitions = df_trx_cached.rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format(partitions ))\n",
    "print(\"Records: {0:,}\".format(df_trx_cached.count()))\n",
    "\n",
    "df_cust_cached = df_cust_cached.repartition(45)\n",
    "\n",
    "partitions = df_cust_cached.rdd.getNumPartitions()\n",
    "print(\"Partitions: {0:,}\".format(partitions ))\n",
    "print(\"Records: {0:,}\".format(df_cust_cached.count()))\n",
    "\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 View your Spark configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'1'),\n",
       " (u'spark.driver.memory', u'2G'),\n",
       " (u'spark.history.kerberos.principal', u'spark-SWEDBANKSEHA2@FSPA.MYNTET.SE'),\n",
       " (u'spark.history.ui.port', u'18081'),\n",
       " (u'spark.driver.extraLibraryPath',\n",
       "  u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " (u'spark.dynamicAllocation.maxExecutors', u'15'),\n",
       " (u'spark.ui.killEnabled', u'true'),\n",
       " (u'spark.history.kerberos.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.executorIdleTimeout', u'60'),\n",
       " (u'spark.executor.extraLibraryPath',\n",
       "  u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " (u'spark.yarn.historyServer.address', u'sb-hdp-m3.fspa.myntet.se:18081'),\n",
       " (u'spark.history.provider',\n",
       "  u'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.dynamicAllocation.initialExecutors', u'1'),\n",
       " (u'spark.executor.memory', u'60G'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.eventLog.dir', u'hdfs:///spark2-history/'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip:/usr/hdp/current/spark2-client/python/<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.4-src.zip'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'sb-hdp-m2.fspa.myntet.se,sb-hdp-m3.fspa.myntet.se'),\n",
       " (u'spark.driver.maxResultSize', u'2G'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.driver.port', u'43037'),\n",
       " (u'spark.driver.appUIAddress', u'http://39.7.48.31:4076'),\n",
       " (u'spark.yarn.queue', u'default'),\n",
       " (u'spark.history.kerberos.keytab',\n",
       "  u'/etc/security/keytabs/spark.headless.keytab'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.port.maxRetries', u'50'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'https://sb-hdp-m2.fspa.myntet.se:8090/proxy/application_1554311896068_9231,https://sb-hdp-m3.fspa.myntet.se:8090/proxy/application_1554311896068_9231'),\n",
       " (u'spark.sql.hive.convertMetastoreOrc', u'false'),\n",
       " (u'spark.app.name', u'pyspark-shell'),\n",
       " (u'spark.history.fs.logDirectory', u'hdfs:///spark2-history/'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.executor.cores', u'15'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.app.id', u'application_1554311896068_9231'),\n",
       " (u'spark.driver.host', u'39.7.48.31'),\n",
       " (u'spark.executor.instances', u'1'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DL pySpark 30 core, 60G memory (Spark 2.2)",
   "language": "python",
   "name": "dl_30core60g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
